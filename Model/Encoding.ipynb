{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d332e1-1087-4a3e-8e22-9c023cc1b939",
   "metadata": {},
   "source": [
    "# Import Libraries, Modules & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9142e454-f430-4b17-9a4c-0839e1e95364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Wranglers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "#Setting\n",
    "%config Completer.use_jedi = False\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ce605525-0e97-4ed9-abe8-3ccf2f05c934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e054dba1-0873-4782-aa04-94c1c2c0d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape = (17996, 17)\n",
      "test.shape  = (7713, 16)\n",
      "sub.shape   = (7713, 11)\n"
     ]
    }
   ],
   "source": [
    "sub, test, train = [ pd.read_csv(_) for _ in glob(\"..\\\\DATA\\\\*\") if \"csv\" in _ ]\n",
    "\n",
    "print(f\"{train.shape = }\")\n",
    "print(f\"{test.shape  = }\")\n",
    "print(f\"{sub.shape   = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d518de6-2ec4-4a45-adf5-e4d130134585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Acollective</td>\n",
       "      <td>Happiest of All Memorial Days</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.276</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.230</td>\n",
       "      <td>147.904</td>\n",
       "      <td>281093.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15847</th>\n",
       "      <td>Dot Cromwell</td>\n",
       "      <td>Hold U</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.336</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.578</td>\n",
       "      <td>133.048</td>\n",
       "      <td>162075.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6159</th>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Hound Dog</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.949</td>\n",
       "      <td>86.895</td>\n",
       "      <td>2.267117</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>Dust Bolt</td>\n",
       "      <td>Bloody Rain</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.970</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.589</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>0.431</td>\n",
       "      <td>145.036</td>\n",
       "      <td>316930.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>That's What I Like (feat. Gucci Mane)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.899</td>\n",
       "      <td>134.071</td>\n",
       "      <td>234596.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17299</th>\n",
       "      <td>Oren Barzilay</td>\n",
       "      <td>◊ê◊ó◊ú◊ï◊ù ◊ú◊†◊¶◊ó - ◊ê◊ß◊ï◊°◊ò◊ô</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.251</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-11.476</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.357</td>\n",
       "      <td>105.969</td>\n",
       "      <td>212696.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>LUX‚Ç¨ BOYZ</td>\n",
       "      <td>DM</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.610</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7.817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.634</td>\n",
       "      <td>135.951</td>\n",
       "      <td>137647.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>A Perfect Circle</td>\n",
       "      <td>The Doomed</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.903</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.239</td>\n",
       "      <td>106.329</td>\n",
       "      <td>281781.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16915</th>\n",
       "      <td>Tenacious D</td>\n",
       "      <td>Tribute</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.767</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-6.185</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>0.422</td>\n",
       "      <td>94.174</td>\n",
       "      <td>248053.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17560</th>\n",
       "      <td>DJ Zinhle</td>\n",
       "      <td>Go!</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.046</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.935</td>\n",
       "      <td>115.010</td>\n",
       "      <td>294261.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artist Name                             Track Name  Popularity  \\\n",
       "242         Acollective          Happiest of All Memorial Days        37.0   \n",
       "15847      Dot Cromwell                                 Hold U        33.0   \n",
       "6159      Elvis Presley                              Hound Dog        67.0   \n",
       "11287         Dust Bolt                            Bloody Rain        21.0   \n",
       "0            Bruno Mars  That's What I Like (feat. Gucci Mane)        60.0   \n",
       "17299     Oren Barzilay     ◊ê◊ó◊ú◊ï◊ù ◊ú◊†◊¶◊ó - ◊ê◊ß◊ï◊°◊ò◊ô        23.0   \n",
       "2466        LUX‚Ç¨ BOYZ                                     DM         9.0   \n",
       "931    A Perfect Circle                             The Doomed        54.0   \n",
       "16915       Tenacious D                                Tribute        68.0   \n",
       "17560         DJ Zinhle                                    Go!        31.0   \n",
       "\n",
       "       danceability  energy   key  loudness  mode  speechiness  acousticness  \\\n",
       "242           0.412   0.510   6.0    -8.276     0       0.0298      0.348000   \n",
       "15847         0.837   0.737   NaN    -3.336     1       0.2750      0.277000   \n",
       "6159          0.494   0.756   NaN    -8.492     1       0.0499      0.733000   \n",
       "11287         0.220   0.970   3.0    -3.589     1       0.0646      0.000017   \n",
       "0             0.854   0.564   1.0    -4.964     1       0.0485      0.017100   \n",
       "17299         0.501   0.251   9.0   -11.476     0       0.0286      0.694000   \n",
       "2466          0.877   0.610  10.0    -7.817     0       0.0731      0.023800   \n",
       "931           0.367   0.903   4.0    -5.956     0       0.1750      0.047700   \n",
       "16915         0.365   0.767   9.0    -6.185     0       0.0681      0.181000   \n",
       "17560         0.803   0.634   1.0   -11.046     1       0.0549      0.016900   \n",
       "\n",
       "       instrumentalness  liveness  valence    tempo  duration_in min/ms  \\\n",
       "242            0.570000    0.0787    0.230  147.904       281093.000000   \n",
       "15847          0.000094    0.3220    0.578  133.048       162075.000000   \n",
       "6159           0.005050    0.7600    0.949   86.895            2.267117   \n",
       "11287          0.000303    0.3390    0.431  145.036       316930.000000   \n",
       "0                   NaN    0.0849    0.899  134.071       234596.000000   \n",
       "17299          0.006000    0.1190    0.357  105.969       212696.000000   \n",
       "2466           0.611000    0.1150    0.634  135.951       137647.000000   \n",
       "931            0.001240    0.2350    0.239  106.329       281781.000000   \n",
       "16915          0.000856    0.0942    0.422   94.174       248053.000000   \n",
       "17560          0.663000    0.0613    0.935  115.010       294261.000000   \n",
       "\n",
       "       time_signature  Class  \n",
       "242                 4      6  \n",
       "15847               4      5  \n",
       "6159                4     10  \n",
       "11287               4      8  \n",
       "0                   4      5  \n",
       "17299               4     10  \n",
       "2466                4      5  \n",
       "931                 3     10  \n",
       "16915               4     10  \n",
       "17560               4      9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d0d9b2-9f8a-40fc-879d-22a9bf2901c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Acoustic/Folk_0',\n",
       " 1: 'Alt_1',\n",
       " 2: 'Blues_2',\n",
       " 3: 'Bollywood_3',\n",
       " 4: 'Country_4',\n",
       " 5: 'HipHop_5',\n",
       " 6: 'Indie Alt_6',\n",
       " 7: 'Instrumental_7',\n",
       " 8: 'Metal_8',\n",
       " 9: 'Pop_9',\n",
       " 10: 'Rock_10'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting Coumn Names For Easy Access.\n",
    "train.columns = train.columns.str.replace(\" \", \"_\")\n",
    "test.columns  = test.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "# Extracting Target Class Names.\n",
    "_ = pd.Series(sub.columns).str.split(\"_\").apply(lambda x: x[0]).values\n",
    "gerne_dict = { num:(name + \"_\" + str(num)) for num,name in enumerate(_) }\n",
    "# Adding Extra Target Name Column For Convinience.\n",
    "train[\"Class_names\"] = train.Class.map(gerne_dict)\n",
    "# The Dict for Map.\n",
    "gerne_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca34a6e-7c38-4d54-828c-027950fd57cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Track_Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in_min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "      <th>Class_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4215</th>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>Crossfire</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.963</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.115</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.4030</td>\n",
       "      <td>175.068</td>\n",
       "      <td>243000.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Metal_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10665</th>\n",
       "      <td>YG</td>\n",
       "      <td>Vibe With You (feat. Ty Dolla $ign)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>91.867</td>\n",
       "      <td>188451.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>HipHop_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867</th>\n",
       "      <td>The Oak Ridge Boys</td>\n",
       "      <td>Elvira</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.285</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-17.491</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>126.946</td>\n",
       "      <td>3.76045</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Country_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Jake Bugg</td>\n",
       "      <td>Lost</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.896</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.512</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>124.013</td>\n",
       "      <td>208402.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Pop_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9038</th>\n",
       "      <td>Stepping Sideways</td>\n",
       "      <td>Quintessence</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.983</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.980</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>160.002</td>\n",
       "      <td>205800.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Metal_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9962</th>\n",
       "      <td>Bury Me Alive</td>\n",
       "      <td>You &amp; I</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.962</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.334</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>140.062</td>\n",
       "      <td>260653.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Metal_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12382</th>\n",
       "      <td>Yuval Mendelson</td>\n",
       "      <td>◊í◊û◊ï◊® ◊ï◊û◊†◊ï◊ô</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.769</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-6.617</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>103.874</td>\n",
       "      <td>156350.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>Rock_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6842</th>\n",
       "      <td>The Desperate Kingdom of Love</td>\n",
       "      <td>Max Jury, Fenne Lily</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.446</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>151.213</td>\n",
       "      <td>2.87615</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Acoustic/Folk_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13046</th>\n",
       "      <td>John Mayer</td>\n",
       "      <td>Waiting On the World to Change</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.756</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.820</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.6630</td>\n",
       "      <td>176.739</td>\n",
       "      <td>201173.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Pop_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>Tandra</td>\n",
       "      <td>Marching to Infinity</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.987</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.011</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>130.032</td>\n",
       "      <td>381120.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Metal_8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Artist_Name                           Track_Name  \\\n",
       "4215                      Santa Cruz                            Crossfire   \n",
       "10665                             YG  Vibe With You (feat. Ty Dolla $ign)   \n",
       "8867              The Oak Ridge Boys                               Elvira   \n",
       "874                        Jake Bugg                                 Lost   \n",
       "9038               Stepping Sideways                         Quintessence   \n",
       "9962                   Bury Me Alive                              You & I   \n",
       "12382                Yuval Mendelson                  ◊í◊û◊ï◊® ◊ï◊û◊†◊ï◊ô   \n",
       "6842   The Desperate Kingdom of Love                 Max Jury, Fenne Lily   \n",
       "13046                     John Mayer       Waiting On the World to Change   \n",
       "2918                          Tandra                 Marching to Infinity   \n",
       "\n",
       "       Popularity  danceability  energy   key  loudness  mode  speechiness  \\\n",
       "4215         40.0         0.405   0.963   6.0    -2.115     0       0.1330   \n",
       "10665         NaN         0.604   0.700   1.0    -6.017     1       0.1240   \n",
       "8867         60.0         0.825   0.285   8.0   -17.491     1       0.0326   \n",
       "874          62.0         0.723   0.896  10.0    -2.512     1       0.0435   \n",
       "9038         20.0         0.323   0.983   2.0    -3.980     0       0.1160   \n",
       "9962         39.0         0.457   0.962   2.0    -3.334     1       0.1590   \n",
       "12382        22.0         0.565   0.769   9.0    -6.617     0       0.0332   \n",
       "6842         36.0         0.375   0.407   NaN    -9.446     1       0.0299   \n",
       "13046        73.0         0.578   0.756   2.0    -5.820     1       0.1150   \n",
       "2918         36.0         0.243   0.987   4.0    -4.011     0       0.2930   \n",
       "\n",
       "       acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "4215       0.000911          0.004760    0.1880   0.4030  175.068   \n",
       "10665      0.156000               NaN    0.1290   0.4620   91.867   \n",
       "8867       0.188000          0.001310    0.0539   0.8600  126.946   \n",
       "874        0.059800          0.000078    0.0689   0.9380  124.013   \n",
       "9038       0.000147          0.000978    0.1160   0.0998  160.002   \n",
       "9962       0.000515          0.000580    0.1320   0.1370  140.062   \n",
       "12382      0.015100               NaN    0.3470   0.6900  103.874   \n",
       "6842       0.037700          0.160000    0.1110   0.1240  151.213   \n",
       "13046      0.157000          0.000232    0.2490   0.6630  176.739   \n",
       "2918       0.001600          0.037200    0.1580   0.1120  130.032   \n",
       "\n",
       "       duration_in_min/ms  time_signature  Class      Class_names  \n",
       "4215         243000.00000               4      8          Metal_8  \n",
       "10665        188451.00000               4      5         HipHop_5  \n",
       "8867              3.76045               4      4        Country_4  \n",
       "874          208402.00000               4      9            Pop_9  \n",
       "9038         205800.00000               4      8          Metal_8  \n",
       "9962         260653.00000               4      8          Metal_8  \n",
       "12382        156350.00000               4     10          Rock_10  \n",
       "6842              2.87615               3      0  Acoustic/Folk_0  \n",
       "13046        201173.00000               4      9            Pop_9  \n",
       "2918         381120.00000               4      8          Metal_8  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a581bd5-4ee7-476e-8c05-6fc0e7cb85c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock_10            4949\n",
       "Indie Alt_6        2587\n",
       "Pop_9              2524\n",
       "Metal_8            1854\n",
       "HipHop_5           1447\n",
       "Alt_1              1373\n",
       "Blues_2            1272\n",
       "Acoustic/Folk_0     625\n",
       "Instrumental_7      576\n",
       "Bollywood_3         402\n",
       "Country_4           387\n",
       "Name: Class_names, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Of All Target Classes\n",
    "train.Class_names.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82c86988-4656-4b23-af0e-f0975813373d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock_10            27.50\n",
       "Indie Alt_6        14.38\n",
       "Pop_9              14.03\n",
       "Metal_8            10.30\n",
       "HipHop_5            8.04\n",
       "Alt_1               7.63\n",
       "Blues_2             7.07\n",
       "Acoustic/Folk_0     3.47\n",
       "Instrumental_7      3.20\n",
       "Bollywood_3         2.23\n",
       "Country_4           2.15\n",
       "Name: Class_names, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % Share of all target classes\n",
    "train.Class_names.value_counts(normalize=True).round(4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52093201-38ed-4e60-bbf0-46d1c91fbad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(1,1, figsize=(20,5))\n",
    "sns.countplot(train.Class_names, ax=ax)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20733573-806a-4ced-87f0-80fbac69e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(labels=[\"Artist_Name\", \"Track_Name\", \"Class\", \"Class_names\"], axis=1)\n",
    "y_train = train.Class\n",
    "\n",
    "X_test  = test.drop(labels=[\"Artist_Name\", \"Track_Name\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf6ccd-38e0-42bb-b97a-503d97be6caa",
   "metadata": {},
   "source": [
    "## Calssification Un-Supervised Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78640577-35e1-40a7-9937-8c46e551a513",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "K-Means requires scaled inputs so by scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f751de3e-6ac7-43e8-b3e5-2aa365c9679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "KM_Pipe =  Pipeline(\n",
    "    \n",
    "    [\n",
    "        (\"scaler\",  QuantileTransformer(output_distribution='uniform',random_state=101,)),\n",
    "        (\"impute\", KNNImputer(n_neighbors=17, weights='distance')),\n",
    "        (\"kmeans\", KMeans(n_clusters=12, random_state=101,max_iter=300, tol=0.00001))\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2806e8-d6c2-401c-b5c8-412a46b508cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', QuantileTransformer(random_state=101)),\n",
       "                ('impute', KNNImputer(n_neighbors=17, weights='distance')),\n",
       "                ('kmeans', KMeans(n_clusters=12, random_state=101, tol=1e-05))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "KM_Pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0509d6d-cdd2-4d43-ad98-ca9b30551b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     1746\n",
       "1     1699\n",
       "8     1653\n",
       "10    1621\n",
       "2     1596\n",
       "3     1517\n",
       "9     1453\n",
       "6     1441\n",
       "11    1387\n",
       "7     1351\n",
       "5     1297\n",
       "4     1235\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "KM_label_train = KM_Pipe.predict(X_train)\n",
    "KM_label_test  = KM_Pipe.predict(X_test)\n",
    "\n",
    "X_train[\"KM_Label\"] = KM_label_train\n",
    "X_test[ \"KM_Label\"]  = KM_label_test\n",
    "\n",
    "pd.Series(KM_label_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5071e-798e-4fa1-9491-fa6575fc5b0f",
   "metadata": {},
   "source": [
    "# Artist Names : Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88776c08-8b80-4aa1-98ef-9fbbe8b0b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_count_dict = pd.concat([train.Artist_Name, test.Artist_Name]).value_counts().to_dict()\n",
    "art_freq_dict  = pd.concat([train.Artist_Name, test.Artist_Name]).value_counts(normalize=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7b97058-bc42-43fc-99bb-182b72c946fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train['Artist'] = train.Artist_Name.map(art_count_dict)\n",
    "X_test['Artist']  = test.Artist_Name.map(art_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bd5ea-e6a0-4b09-ad70-63d6a8c32e94",
   "metadata": {},
   "source": [
    "# Strainght XGBoost\n",
    "\n",
    "## Iteration-1 : Best Param\n",
    "xgb_param_dict = {\n",
    "    \n",
    "    \"n_estimators\"     : 43, #The Third Iteration (39 & 38 Found To be Best Depth 5) (1st: 29 & 28 Found To be Best Depth 6)\n",
    "    \"max_depth\"        : 5,  # Max Depth 5 Selected \n",
    "    \"learning_rate\"    : 0.275,\n",
    "    \"use_label_encoder\": False,\n",
    "    \"n_jobs\"           : 4,\n",
    "    \"random_state\"     : 40,\n",
    "    \"verbosity\"        : 1,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92bdbe49-c222-4a98-a876-0a26f77ba918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2910b888-ba37-42c5-94a8-6042303d21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 14.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.275, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=50, n_jobs=4, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=40, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_param_dict = {\n",
    "    \n",
    "    \"n_estimators\"     : 50, #The Third Iteration (39 & 38 Found To be Best Depth 5) (1st: 29 & 28 Found To be Best Depth 6)\n",
    "    \"max_depth\"        : 4,  # Max Depth 5 Selected \n",
    "    \"learning_rate\"    : 0.3,\n",
    "    \"use_label_encoder\": False,\n",
    "    \"n_jobs\"           : 4,\n",
    "    \"random_state\"     : 0,\n",
    "    \"verbosity\"        : 1,\n",
    "    \n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_param_dict)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fde9e72e-d1a9-4549-8e09-e9f877879c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 347 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duration_in_min/ms    14.418909\n",
       "acousticness          12.121751\n",
       "speechiness           11.216524\n",
       "instrumentalness       8.671970\n",
       "energy                 8.620054\n",
       "Artist                 8.106141\n",
       "danceability           7.367527\n",
       "valence                5.444935\n",
       "Popularity             5.336744\n",
       "time_signature         3.770521\n",
       "mode                   3.271279\n",
       "KM_Label               3.117625\n",
       "loudness               3.090051\n",
       "liveness               2.119473\n",
       "tempo                  2.015083\n",
       "key                    1.311415\n",
       "dtype: float32"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Prediction\n",
    "y_pred = pd.DataFrame(xgb_model.predict_proba(X_test), index = X_test.index)\n",
    "# Submission Formatting\n",
    "y_pred.columns = y_pred.columns.map(gerne_dict)\n",
    "# Feature Importance\n",
    "pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)*100"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23b70ae3-5da8-4039-8e04-2fa8e8b8ad28",
   "metadata": {},
   "source": [
    "# Write To CSV For Submission. (Change Cell from RAW 2 CODE for exicuting action)\n",
    "\n",
    "y_pred.to_csv(\"less_light_music_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05219f3c-1f50-4ab6-b299-29187d11266d",
   "metadata": {},
   "source": [
    "# GSCV - First Optimization : ***n_estimators***\n",
    "## Target: To Keep Model Light Weight\n",
    "\n",
    "- Iteration 1: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "12441b6c-1bd8-42ea-8b2f-1e3aa6e71eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 994 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    \n",
    "    \"n_estimators\"     : range(45, 55),\n",
    "    \"colsample_bytree\" : [ 0.5, 0.6, 0.7, 0.8, 0.9, 1 ],\n",
    "    \"n_jobs\"           : [ 4 ],\n",
    "    \"random_state\"     : [ 0 ],\n",
    "    \"verbosity\"        : [ 1 ],\n",
    "}\n",
    "\n",
    "gscv_xgb = GridSearchCV(\n",
    "    \n",
    "    XGBClassifier(**xgb_param_dict),\n",
    "    param_grid,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs  = None,\n",
    "    refit   = True,\n",
    "    cv      = cv,\n",
    "    verbose = 1,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "86ab1c2e-e295-426e-a234-75019241bd45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 60 candidates, totalling 480 fits\n",
      "[20:52:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:52:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:52:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:55:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:58:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:00:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:03:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:04:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:05:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:07:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:09:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:10:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:12:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:15:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:17:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:18:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:19:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:20:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:22:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:22:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:22:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:23:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:24:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:24:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:24:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:24:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:25:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:25:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:26:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:26:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:26:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:26:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:28:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:30:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:31:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:33:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:33:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:33:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:33:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:33:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:33:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:34:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:34:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:34:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:34:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:34:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:35:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:35:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:35:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:35:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:35:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:36:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:36:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:36:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:36:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:36:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:36:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:37:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:40:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:41:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:42:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:43:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:43:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:43:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:43:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:43:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:44:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:44:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:44:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:44:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:44:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:46:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:46:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:46:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:46:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:46:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:47:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:47:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:47:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:48:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:48:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:48:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:48:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:48:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:50:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:51:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:52:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:52:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:52:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:52:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:52:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:52:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:53:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:53:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:53:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:53:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:53:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:54:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:54:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:54:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:54:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:54:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:55:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:55:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:55:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:55:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:55:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:56:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:56:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:56:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:56:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:56:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:56:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:57:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:57:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:57:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:57:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:57:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:58:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:58:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:58:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:58:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:58:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:58:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:59:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:59:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:59:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:59:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:59:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:59:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:01:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:01:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:01:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:01:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:02:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:02:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:02:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:02:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:02:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 1h 10min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=2, n_splits=4, random_state=57),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=0.275, max_delta_step=None,\n",
       "                                     max_depth=4, min_child_weight=None,\n",
       "                                     m...\n",
       "                                     num_parallel_tree=None, random_state=40,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, use_label_encoder=False,\n",
       "                                     validate_parameters=None, verbosity=1),\n",
       "             param_grid={'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
       "                         'n_estimators': range(45, 55), 'n_jobs': [4],\n",
       "                         'random_state': [0], 'verbosity': [1]},\n",
       "             scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gscv_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "be81610d-5b13-4954-8624-1da33f02b941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_n_jobs</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>param_verbosity</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.314784</td>\n",
       "      <td>1.074151</td>\n",
       "      <td>0.068106</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.8</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'n_estimators': 54, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.123324</td>\n",
       "      <td>-1.125631</td>\n",
       "      <td>-1.122516</td>\n",
       "      <td>-1.118647</td>\n",
       "      <td>-1.129964</td>\n",
       "      <td>-1.118719</td>\n",
       "      <td>-1.125163</td>\n",
       "      <td>-1.124290</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.775069</td>\n",
       "      <td>1.630037</td>\n",
       "      <td>0.075377</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>0.8</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'n_estimators': 53, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.123487</td>\n",
       "      <td>-1.125813</td>\n",
       "      <td>-1.123803</td>\n",
       "      <td>-1.118621</td>\n",
       "      <td>-1.130225</td>\n",
       "      <td>-1.119189</td>\n",
       "      <td>-1.125369</td>\n",
       "      <td>-1.124635</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.312456</td>\n",
       "      <td>0.867478</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>0.9</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'n_estimators': 54, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.118430</td>\n",
       "      <td>-1.125214</td>\n",
       "      <td>-1.125947</td>\n",
       "      <td>-1.118958</td>\n",
       "      <td>-1.131522</td>\n",
       "      <td>-1.118690</td>\n",
       "      <td>-1.130348</td>\n",
       "      <td>-1.125201</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10.261982</td>\n",
       "      <td>0.478006</td>\n",
       "      <td>0.069273</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'n_estimators': 52, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.124393</td>\n",
       "      <td>-1.126462</td>\n",
       "      <td>-1.124049</td>\n",
       "      <td>-1.119776</td>\n",
       "      <td>-1.130721</td>\n",
       "      <td>-1.120063</td>\n",
       "      <td>-1.125789</td>\n",
       "      <td>-1.125299</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11.752255</td>\n",
       "      <td>1.053812</td>\n",
       "      <td>0.079837</td>\n",
       "      <td>0.019034</td>\n",
       "      <td>0.7</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'n_estimators': 54, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.125463</td>\n",
       "      <td>-1.126668</td>\n",
       "      <td>-1.122005</td>\n",
       "      <td>-1.120498</td>\n",
       "      <td>-1.129334</td>\n",
       "      <td>-1.120028</td>\n",
       "      <td>-1.129313</td>\n",
       "      <td>-1.125422</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10.964720</td>\n",
       "      <td>0.838937</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 1, 'n_estimators': 54, 'n...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.122305</td>\n",
       "      <td>-1.129704</td>\n",
       "      <td>-1.123490</td>\n",
       "      <td>-1.120408</td>\n",
       "      <td>-1.125640</td>\n",
       "      <td>-1.121668</td>\n",
       "      <td>-1.128477</td>\n",
       "      <td>-1.125511</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9.375297</td>\n",
       "      <td>0.569826</td>\n",
       "      <td>0.051543</td>\n",
       "      <td>0.014246</td>\n",
       "      <td>0.9</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'n_estimators': 53, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.118639</td>\n",
       "      <td>-1.124904</td>\n",
       "      <td>-1.125991</td>\n",
       "      <td>-1.119510</td>\n",
       "      <td>-1.131604</td>\n",
       "      <td>-1.119722</td>\n",
       "      <td>-1.131133</td>\n",
       "      <td>-1.125558</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "39      10.314784      1.074151         0.068106        0.008086   \n",
       "38      10.775069      1.630037         0.075377        0.014495   \n",
       "49       9.312456      0.867478         0.055970        0.012369   \n",
       "37      10.261982      0.478006         0.069273        0.008554   \n",
       "29      11.752255      1.053812         0.079837        0.019034   \n",
       "59      10.964720      0.838937         0.058157        0.009961   \n",
       "48       9.375297      0.569826         0.051543        0.014246   \n",
       "\n",
       "   param_colsample_bytree param_n_estimators param_n_jobs param_random_state  \\\n",
       "39                    0.8                 54            4                  0   \n",
       "38                    0.8                 53            4                  0   \n",
       "49                    0.9                 54            4                  0   \n",
       "37                    0.8                 52            4                  0   \n",
       "29                    0.7                 54            4                  0   \n",
       "59                      1                 54            4                  0   \n",
       "48                    0.9                 53            4                  0   \n",
       "\n",
       "   param_verbosity                                             params  ...  \\\n",
       "39               1  {'colsample_bytree': 0.8, 'n_estimators': 54, ...  ...   \n",
       "38               1  {'colsample_bytree': 0.8, 'n_estimators': 53, ...  ...   \n",
       "49               1  {'colsample_bytree': 0.9, 'n_estimators': 54, ...  ...   \n",
       "37               1  {'colsample_bytree': 0.8, 'n_estimators': 52, ...  ...   \n",
       "29               1  {'colsample_bytree': 0.7, 'n_estimators': 54, ...  ...   \n",
       "59               1  {'colsample_bytree': 1, 'n_estimators': 54, 'n...  ...   \n",
       "48               1  {'colsample_bytree': 0.9, 'n_estimators': 53, ...  ...   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "39          -1.123324          -1.125631          -1.122516   \n",
       "38          -1.123487          -1.125813          -1.123803   \n",
       "49          -1.118430          -1.125214          -1.125947   \n",
       "37          -1.124393          -1.126462          -1.124049   \n",
       "29          -1.125463          -1.126668          -1.122005   \n",
       "59          -1.122305          -1.129704          -1.123490   \n",
       "48          -1.118639          -1.124904          -1.125991   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "39          -1.118647          -1.129964          -1.118719   \n",
       "38          -1.118621          -1.130225          -1.119189   \n",
       "49          -1.118958          -1.131522          -1.118690   \n",
       "37          -1.119776          -1.130721          -1.120063   \n",
       "29          -1.120498          -1.129334          -1.120028   \n",
       "59          -1.120408          -1.125640          -1.121668   \n",
       "48          -1.119510          -1.131604          -1.119722   \n",
       "\n",
       "    split7_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "39          -1.125163        -1.124290        0.004161                1  \n",
       "38          -1.125369        -1.124635        0.004128                2  \n",
       "49          -1.130348        -1.125201        0.005562                3  \n",
       "37          -1.125789        -1.125299        0.003959                4  \n",
       "29          -1.129313        -1.125422        0.003850                5  \n",
       "59          -1.128477        -1.125511        0.004016                6  \n",
       "48          -1.131133        -1.125558        0.005495                7  \n",
       "\n",
       "[7 rows x 21 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_xgb.cv_results_).sort_values(by=\"rank_test_score\").head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ee9b7-2806-4149-924d-8aac085f10ab",
   "metadata": {},
   "source": [
    "# GSCV - Second Optimization : max_depth\n",
    "- Iteration 1 : { 'n_estimators' : 50 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "645fb9ed-687f-4102-b3ab-49c7399b6363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid_2 = {\n",
    "    \n",
    "    \"n_estimators\"     : [ 54 ],\n",
    "    \"colsample_bytree\" : [ 0.8 ],\n",
    "    \"max_depth\"        : range(2, 9),\n",
    "    \"n_jobs\"           : [ 4 ],\n",
    "    \"random_state\"     : [ 0 ],\n",
    "    \"verbosity\"        : [ 1 ],\n",
    "}\n",
    "\n",
    "gscv_xgb_2 = GridSearchCV(\n",
    "    \n",
    "    XGBClassifier(**xgb_param_dict),\n",
    "    param_grid_2,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs  = None,\n",
    "    refit   = True,\n",
    "    cv      = cv,\n",
    "    verbose = 1,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f454390-1cbf-4851-9ed6-899c15e5d917",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 7 candidates, totalling 56 fits\n",
      "[22:35:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:36:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:37:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:38:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:38:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:38:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:38:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:38:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:39:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:39:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:39:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:39:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:40:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:40:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:41:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:41:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:41:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:41:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:42:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:42:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:42:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:42:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:43:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:43:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:44:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:44:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:44:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:44:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:45:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:45:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:45:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:46:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:46:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:47:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:48:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:48:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 12min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=2, n_splits=4, random_state=57),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=0.275, max_delta_step=None,\n",
       "                                     max_depth=4, min_child_weight=None,\n",
       "                                     m...\n",
       "                                     n_estimators=50, n_jobs=4,\n",
       "                                     num_parallel_tree=None, random_state=40,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, use_label_encoder=False,\n",
       "                                     validate_parameters=None, verbosity=1),\n",
       "             param_grid={'colsample_bytree': [0.8], 'max_depth': range(2, 9),\n",
       "                         'n_estimators': [54], 'n_jobs': [4],\n",
       "                         'random_state': [0], 'verbosity': [1]},\n",
       "             scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gscv_xgb_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a53ac5d0-bd6c-44a5-9196-1e3929c507d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_n_jobs</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>param_verbosity</th>\n",
       "      <th>...</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.667453</td>\n",
       "      <td>1.696429</td>\n",
       "      <td>0.063412</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.123324</td>\n",
       "      <td>-1.125631</td>\n",
       "      <td>-1.122516</td>\n",
       "      <td>-1.118647</td>\n",
       "      <td>-1.129964</td>\n",
       "      <td>-1.118719</td>\n",
       "      <td>-1.125163</td>\n",
       "      <td>-1.124290</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.816640</td>\n",
       "      <td>3.133641</td>\n",
       "      <td>0.104530</td>\n",
       "      <td>0.024743</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126128</td>\n",
       "      <td>-1.128791</td>\n",
       "      <td>-1.131617</td>\n",
       "      <td>-1.121999</td>\n",
       "      <td>-1.139280</td>\n",
       "      <td>-1.120562</td>\n",
       "      <td>-1.125694</td>\n",
       "      <td>-1.128378</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.934630</td>\n",
       "      <td>0.798475</td>\n",
       "      <td>0.050812</td>\n",
       "      <td>0.010093</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.139067</td>\n",
       "      <td>-1.140723</td>\n",
       "      <td>-1.143781</td>\n",
       "      <td>-1.135264</td>\n",
       "      <td>-1.145963</td>\n",
       "      <td>-1.135640</td>\n",
       "      <td>-1.145109</td>\n",
       "      <td>-1.141251</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.024620</td>\n",
       "      <td>2.520423</td>\n",
       "      <td>0.085899</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.129016</td>\n",
       "      <td>-1.147047</td>\n",
       "      <td>-1.139970</td>\n",
       "      <td>-1.141931</td>\n",
       "      <td>-1.154095</td>\n",
       "      <td>-1.132952</td>\n",
       "      <td>-1.140559</td>\n",
       "      <td>-1.142183</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.009265</td>\n",
       "      <td>1.651992</td>\n",
       "      <td>0.104137</td>\n",
       "      <td>0.026732</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160335</td>\n",
       "      <td>-1.186487</td>\n",
       "      <td>-1.175544</td>\n",
       "      <td>-1.168808</td>\n",
       "      <td>-1.192853</td>\n",
       "      <td>-1.156456</td>\n",
       "      <td>-1.169281</td>\n",
       "      <td>-1.173848</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "2       9.667453      1.696429         0.063412        0.015119   \n",
       "3      14.816640      3.133641         0.104530        0.024743   \n",
       "1       6.934630      0.798475         0.050812        0.010093   \n",
       "4      16.024620      2.520423         0.085899        0.009130   \n",
       "5      20.009265      1.651992         0.104137        0.026732   \n",
       "\n",
       "  param_colsample_bytree param_max_depth param_n_estimators param_n_jobs  \\\n",
       "2                    0.8               4                 54            4   \n",
       "3                    0.8               5                 54            4   \n",
       "1                    0.8               3                 54            4   \n",
       "4                    0.8               6                 54            4   \n",
       "5                    0.8               7                 54            4   \n",
       "\n",
       "  param_random_state param_verbosity  ... split1_test_score  \\\n",
       "2                  0               1  ...         -1.123324   \n",
       "3                  0               1  ...         -1.126128   \n",
       "1                  0               1  ...         -1.139067   \n",
       "4                  0               1  ...         -1.129016   \n",
       "5                  0               1  ...         -1.160335   \n",
       "\n",
       "   split2_test_score  split3_test_score  split4_test_score  split5_test_score  \\\n",
       "2          -1.125631          -1.122516          -1.118647          -1.129964   \n",
       "3          -1.128791          -1.131617          -1.121999          -1.139280   \n",
       "1          -1.140723          -1.143781          -1.135264          -1.145963   \n",
       "4          -1.147047          -1.139970          -1.141931          -1.154095   \n",
       "5          -1.186487          -1.175544          -1.168808          -1.192853   \n",
       "\n",
       "   split6_test_score  split7_test_score  mean_test_score  std_test_score  \\\n",
       "2          -1.118719          -1.125163        -1.124290        0.004161   \n",
       "3          -1.120562          -1.125694        -1.128378        0.005750   \n",
       "1          -1.135640          -1.145109        -1.141251        0.003972   \n",
       "4          -1.132952          -1.140559        -1.142183        0.008108   \n",
       "5          -1.156456          -1.169281        -1.173848        0.011761   \n",
       "\n",
       "   rank_test_score  \n",
       "2                1  \n",
       "3                2  \n",
       "1                3  \n",
       "4                4  \n",
       "5                5  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_xgb_2.cv_results_).sort_values(by=\"rank_test_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a008b77-75ba-4760-9398-ac5a2f385c0f",
   "metadata": {},
   "source": [
    "# Final Optimization: Optuna\n",
    "- Objective beat Baseline: -1.127373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3257262-a9c1-41b3-9124-0b01a2996462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for time calcualtion.\n",
    "def time_it(*, h=0, m=0, s=0):\n",
    "    \"\"\"Calculates timespan in seconds.\"\"\"\n",
    "    time_delta = (60 * 60 * h) + (60 * m) + (s)\n",
    "    ret        = int(time_delta)\n",
    "    return ret if ( ret > 60 ) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "893f721f-6588-4c8d-baba-7be6f95eb38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "497119c6-122b-4bb7-8a61-63e8fbd71b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oprimizer(trial):\n",
    "    \n",
    "    param_dict = {\n",
    "        \n",
    "        # Fixed Variables\n",
    "        \"n_estimators\"     : trial.suggest_int('n_estimators'      , 30 , 70),\n",
    "        \"colsample_bytree\" : trial.suggest_float('colsample_bytree', 0.2, 1 ),\n",
    "        \"max_depth\"        : trial.suggest_int('max_depth'         , 2  , 10),\n",
    "        \"n_jobs\"           : 4,\n",
    "        \"random_state\"     : 0,\n",
    "        \"verbosity\"        : 0,\n",
    "        # Varaibels to be Optimizez\n",
    "        \"learning_rate\"    : trial.suggest_float('learning_rate', 0.00005, 0.9),\n",
    "        \"reg_alpha\"        : trial.suggest_int('reg_alpha'      , 0, 100),\n",
    "        \"reg_lambda\"       : trial.suggest_int('reg_lambda'     , 0, 100),\n",
    "        \"subsample\"        : trial.suggest_float('subsample'    , 0.2, 1),\n",
    "    }\n",
    "    \n",
    "    objective = cross_val_score(XGBClassifier(**param_dict), X_train, y_train, scoring='neg_log_loss', cv=cv, n_jobs=None, verbose=0).mean()\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "191a3282-8df7-44c9-a622-0bdcf4b0b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-11 00:49:57,820]\u001b[0m A new study created in memory with name: no-name-e33aa2a5-868e-47ab-922a-ce8bc5652ee4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9f768eba-aab6-40f6-b0f2-32521e8629b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-11 00:51:08,174]\u001b[0m Trial 0 finished with value: -1.2846585776560402 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.7213504359972172, 'max_depth': 9, 'learning_rate': 0.43105359896820217, 'reg_alpha': 51, 'reg_lambda': 23, 'subsample': 0.5464823743494196}. Best is trial 0 with value: -1.2846585776560402.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:51:31,876]\u001b[0m Trial 1 finished with value: -2.204096570104248 and parameters: {'n_estimators': 41, 'colsample_bytree': 0.29437077777874976, 'max_depth': 5, 'learning_rate': 0.015580675620445468, 'reg_alpha': 94, 'reg_lambda': 43, 'subsample': 0.3213891016608793}. Best is trial 0 with value: -1.2846585776560402.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:53:46,635]\u001b[0m Trial 2 finished with value: -1.174660126876078 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.5079942319768205, 'max_depth': 7, 'learning_rate': 0.1243398063929987, 'reg_alpha': 1, 'reg_lambda': 28, 'subsample': 0.9203306277938872}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:55:16,712]\u001b[0m Trial 3 finished with value: -1.1960665175142318 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.3468403931580657, 'max_depth': 9, 'learning_rate': 0.4305493627427647, 'reg_alpha': 16, 'reg_lambda': 44, 'subsample': 0.8717299149255191}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:56:12,563]\u001b[0m Trial 4 finished with value: -1.3057059431150488 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.4770534813559623, 'max_depth': 8, 'learning_rate': 0.7473226844094054, 'reg_alpha': 71, 'reg_lambda': 45, 'subsample': 0.7943152785963759}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:56:42,893]\u001b[0m Trial 5 finished with value: -2.1066935577606407 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.2354655590269432, 'max_depth': 8, 'learning_rate': 0.01900460388562772, 'reg_alpha': 51, 'reg_lambda': 18, 'subsample': 0.25142690728622896}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:57:17,996]\u001b[0m Trial 6 finished with value: -1.3750573144962746 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.5562912042442325, 'max_depth': 3, 'learning_rate': 0.41053435900057395, 'reg_alpha': 51, 'reg_lambda': 10, 'subsample': 0.36732438427311004}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:57:53,201]\u001b[0m Trial 7 finished with value: -1.3967707442920005 and parameters: {'n_estimators': 34, 'colsample_bytree': 0.3839509464415485, 'max_depth': 10, 'learning_rate': 0.36496105667852463, 'reg_alpha': 50, 'reg_lambda': 55, 'subsample': 0.44098922032271504}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:58:51,400]\u001b[0m Trial 8 finished with value: -1.2281446804380516 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.5362782873692831, 'max_depth': 5, 'learning_rate': 0.8823458616182837, 'reg_alpha': 2, 'reg_lambda': 55, 'subsample': 0.2954949784229431}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 00:59:13,220]\u001b[0m Trial 9 finished with value: -1.5401413828676067 and parameters: {'n_estimators': 41, 'colsample_bytree': 0.45664119242779977, 'max_depth': 5, 'learning_rate': 0.725476519610271, 'reg_alpha': 87, 'reg_lambda': 1, 'subsample': 0.3722962798767746}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:00:08,416]\u001b[0m Trial 10 finished with value: -1.270331404170228 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.9544952939503353, 'max_depth': 2, 'learning_rate': 0.1829293311778474, 'reg_alpha': 21, 'reg_lambda': 95, 'subsample': 0.9887572469911492}. Best is trial 2 with value: -1.174660126876078.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:01:58,712]\u001b[0m Trial 11 finished with value: -1.167397265607451 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.7117734646079567, 'max_depth': 7, 'learning_rate': 0.18758126339410897, 'reg_alpha': 2, 'reg_lambda': 74, 'subsample': 0.9863002016546152}. Best is trial 11 with value: -1.167397265607451.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:03:51,527]\u001b[0m Trial 12 finished with value: -1.1742532707286686 and parameters: {'n_estimators': 46, 'colsample_bytree': 0.7289141539993389, 'max_depth': 7, 'learning_rate': 0.19834104110931805, 'reg_alpha': 1, 'reg_lambda': 86, 'subsample': 0.986187235576107}. Best is trial 11 with value: -1.167397265607451.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:05:01,587]\u001b[0m Trial 13 finished with value: -1.234412632597754 and parameters: {'n_estimators': 43, 'colsample_bytree': 0.7547814901631125, 'max_depth': 6, 'learning_rate': 0.2436534242197238, 'reg_alpha': 25, 'reg_lambda': 91, 'subsample': 0.7306152441883316}. Best is trial 11 with value: -1.167397265607451.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:06:44,560]\u001b[0m Trial 14 finished with value: -1.1477938643330377 and parameters: {'n_estimators': 48, 'colsample_bytree': 0.7341453186640471, 'max_depth': 7, 'learning_rate': 0.27516550544329865, 'reg_alpha': 0, 'reg_lambda': 77, 'subsample': 0.9870226620223261}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:07:55,194]\u001b[0m Trial 15 finished with value: -1.2098356275040898 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.8891994294147096, 'max_depth': 6, 'learning_rate': 0.5598381843060298, 'reg_alpha': 34, 'reg_lambda': 75, 'subsample': 0.7070256304136195}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:09:03,363]\u001b[0m Trial 16 finished with value: -1.1801463724277537 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.6527764190214335, 'max_depth': 4, 'learning_rate': 0.2830204514616558, 'reg_alpha': 12, 'reg_lambda': 71, 'subsample': 0.8627278342869299}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:10:08,948]\u001b[0m Trial 17 finished with value: -1.460238572190201 and parameters: {'n_estimators': 32, 'colsample_bytree': 0.8636490531042822, 'max_depth': 7, 'learning_rate': 0.0880660918856517, 'reg_alpha': 37, 'reg_lambda': 70, 'subsample': 0.9996292668543029}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:11:48,307]\u001b[0m Trial 18 finished with value: -1.1792348627538798 and parameters: {'n_estimators': 38, 'colsample_bytree': 0.8184607150195469, 'max_depth': 10, 'learning_rate': 0.5571662108122164, 'reg_alpha': 8, 'reg_lambda': 81, 'subsample': 0.6381351205172663}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:13:05,492]\u001b[0m Trial 19 finished with value: -1.2754449794798837 and parameters: {'n_estimators': 48, 'colsample_bytree': 0.6368046182030215, 'max_depth': 8, 'learning_rate': 0.31018300501240936, 'reg_alpha': 64, 'reg_lambda': 63, 'subsample': 0.928326610646897}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:14:59,967]\u001b[0m Trial 20 finished with value: -1.195550707522341 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.9639764816543921, 'max_depth': 9, 'learning_rate': 0.5263108962896315, 'reg_alpha': 31, 'reg_lambda': 98, 'subsample': 0.8123788744198119}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:16:36,751]\u001b[0m Trial 21 finished with value: -1.1718414394425283 and parameters: {'n_estimators': 45, 'colsample_bytree': 0.7140199462680624, 'max_depth': 7, 'learning_rate': 0.20115992458401574, 'reg_alpha': 0, 'reg_lambda': 86, 'subsample': 0.994676548736631}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:18:15,032]\u001b[0m Trial 22 finished with value: -1.1990660615897775 and parameters: {'n_estimators': 45, 'colsample_bytree': 0.7936066562320289, 'max_depth': 7, 'learning_rate': 0.15125363446122408, 'reg_alpha': 0, 'reg_lambda': 82, 'subsample': 0.9372867221892721}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:19:18,131]\u001b[0m Trial 23 finished with value: -1.5102002720623235 and parameters: {'n_estimators': 37, 'colsample_bytree': 0.6778666668449402, 'max_depth': 6, 'learning_rate': 0.06112415717640521, 'reg_alpha': 8, 'reg_lambda': 63, 'subsample': 0.8573289694652391}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:21:01,479]\u001b[0m Trial 24 finished with value: -1.1742169284539152 and parameters: {'n_estimators': 50, 'colsample_bytree': 0.6404870745066689, 'max_depth': 8, 'learning_rate': 0.3302333103460182, 'reg_alpha': 17, 'reg_lambda': 78, 'subsample': 0.9779487377914835}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:22:44,333]\u001b[0m Trial 25 finished with value: -1.1790142721338888 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.5780456332041846, 'max_depth': 7, 'learning_rate': 0.2484754832872117, 'reg_alpha': 7, 'reg_lambda': 100, 'subsample': 0.760768342381347}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:24:35,376]\u001b[0m Trial 26 finished with value: -1.210701256947272 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.8782721950251084, 'max_depth': 6, 'learning_rate': 0.21011558464200125, 'reg_alpha': 24, 'reg_lambda': 90, 'subsample': 0.9083154934268707}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:25:43,618]\u001b[0m Trial 27 finished with value: -1.262495180221399 and parameters: {'n_estimators': 46, 'colsample_bytree': 0.785514976693351, 'max_depth': 5, 'learning_rate': 0.1171303882492723, 'reg_alpha': 1, 'reg_lambda': 67, 'subsample': 0.6249957468018551}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:27:42,003]\u001b[0m Trial 28 finished with value: -1.1662625154143382 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.7002236376873922, 'max_depth': 8, 'learning_rate': 0.35531580469980445, 'reg_alpha': 13, 'reg_lambda': 86, 'subsample': 0.8157206420078861}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:28:50,494]\u001b[0m Trial 29 finished with value: -1.2633967943438629 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.697933649152683, 'max_depth': 9, 'learning_rate': 0.5004464373273787, 'reg_alpha': 40, 'reg_lambda': 57, 'subsample': 0.48501965506894895}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:30:31,292]\u001b[0m Trial 30 finished with value: -1.1865742875430356 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.6180472503515487, 'max_depth': 8, 'learning_rate': 0.373132769142482, 'reg_alpha': 14, 'reg_lambda': 75, 'subsample': 0.5519172681893648}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:32:22,299]\u001b[0m Trial 31 finished with value: -1.1656662020694122 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.7229014575598267, 'max_depth': 7, 'learning_rate': 0.2643145352632879, 'reg_alpha': 11, 'reg_lambda': 84, 'subsample': 0.9894993461502569}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:34:41,205]\u001b[0m Trial 32 finished with value: -1.1645099613713854 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.7588071054620232, 'max_depth': 8, 'learning_rate': 0.29549524935492755, 'reg_alpha': 10, 'reg_lambda': 91, 'subsample': 0.833113505493773}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:36:43,349]\u001b[0m Trial 33 finished with value: -1.1967390629239951 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8347734245819551, 'max_depth': 8, 'learning_rate': 0.2811135212760685, 'reg_alpha': 29, 'reg_lambda': 93, 'subsample': 0.8340340773516927}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:38:50,178]\u001b[0m Trial 34 finished with value: -1.1707658139506143 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.766493845708418, 'max_depth': 9, 'learning_rate': 0.45905890045548453, 'reg_alpha': 20, 'reg_lambda': 85, 'subsample': 0.8918019290570552}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:41:05,449]\u001b[0m Trial 35 finished with value: -1.2373830645216186 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.9329912042949838, 'max_depth': 9, 'learning_rate': 0.36880250564318934, 'reg_alpha': 43, 'reg_lambda': 100, 'subsample': 0.6682650293188913}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:42:59,320]\u001b[0m Trial 36 finished with value: -1.1705149741555914 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.7488474867008807, 'max_depth': 8, 'learning_rate': 0.32552721561061043, 'reg_alpha': 12, 'reg_lambda': 90, 'subsample': 0.7824821493168473}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:45:10,481]\u001b[0m Trial 37 finished with value: -1.1752065128466527 and parameters: {'n_estimators': 48, 'colsample_bytree': 0.595888350039536, 'max_depth': 10, 'learning_rate': 0.45381529220638084, 'reg_alpha': 7, 'reg_lambda': 33, 'subsample': 0.942129377842826}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:46:34,981]\u001b[0m Trial 38 finished with value: -1.282110449139283 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8254878069053753, 'max_depth': 9, 'learning_rate': 0.40287453012862695, 'reg_alpha': 62, 'reg_lambda': 83, 'subsample': 0.7028928462443504}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:48:24,245]\u001b[0m Trial 39 finished with value: -1.1926412886230326 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.6934162114620667, 'max_depth': 8, 'learning_rate': 0.25785732571295217, 'reg_alpha': 17, 'reg_lambda': 96, 'subsample': 0.8167108562979537}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:50:13,469]\u001b[0m Trial 40 finished with value: -1.778364185623909 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.668827770849919, 'max_depth': 7, 'learning_rate': 0.01863174224136588, 'reg_alpha': 27, 'reg_lambda': 48, 'subsample': 0.8859066088439161}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:51:59,144]\u001b[0m Trial 41 finished with value: -1.2015874666035766 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.7233115976905122, 'max_depth': 7, 'learning_rate': 0.14579694148902075, 'reg_alpha': 5, 'reg_lambda': 72, 'subsample': 0.9436936987477457}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:53:54,830]\u001b[0m Trial 42 finished with value: -1.1697643674604423 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.5218619966475345, 'max_depth': 6, 'learning_rate': 0.3223806782995764, 'reg_alpha': 11, 'reg_lambda': 77, 'subsample': 0.9689519944992806}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:56:12,075]\u001b[0m Trial 43 finished with value: -1.1550777087549182 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.7866143575565644, 'max_depth': 7, 'learning_rate': 0.24679777158388602, 'reg_alpha': 4, 'reg_lambda': 66, 'subsample': 0.7524139545869276}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:57:41,246]\u001b[0m Trial 44 finished with value: -1.3563481049426271 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.7851981550678269, 'max_depth': 8, 'learning_rate': 0.23395686768669569, 'reg_alpha': 96, 'reg_lambda': 64, 'subsample': 0.7825355146566563}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 01:59:21,905]\u001b[0m Trial 45 finished with value: -1.1911691765988812 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.9070708268279155, 'max_depth': 7, 'learning_rate': 0.39457134019584306, 'reg_alpha': 21, 'reg_lambda': 88, 'subsample': 0.5664843683564611}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:00:48,932]\u001b[0m Trial 46 finished with value: -1.322105427443398 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8328973728373322, 'max_depth': 8, 'learning_rate': 0.2945160979250041, 'reg_alpha': 81, 'reg_lambda': 58, 'subsample': 0.7452664196015877}. Best is trial 14 with value: -1.1477938643330377.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:02:17,653]\u001b[0m Trial 47 finished with value: -1.1456845214107174 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.7394786212094148, 'max_depth': 5, 'learning_rate': 0.3596823401855185, 'reg_alpha': 4, 'reg_lambda': 79, 'subsample': 0.8635175047913325}. Best is trial 47 with value: -1.1456845214107174.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:03:50,434]\u001b[0m Trial 48 finished with value: -1.1449120917117275 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.7507886839794052, 'max_depth': 5, 'learning_rate': 0.4431552826693587, 'reg_alpha': 4, 'reg_lambda': 78, 'subsample': 0.8580222150250285}. Best is trial 48 with value: -1.1449120917117275.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:04:59,743]\u001b[0m Trial 49 finished with value: -1.1427021471671321 and parameters: {'n_estimators': 43, 'colsample_bytree': 0.8043674912191477, 'max_depth': 4, 'learning_rate': 0.5000548917511728, 'reg_alpha': 4, 'reg_lambda': 40, 'subsample': 0.8478585601045516}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:06:12,966]\u001b[0m Trial 50 finished with value: -1.145817215446653 and parameters: {'n_estimators': 43, 'colsample_bytree': 0.913928315336396, 'max_depth': 4, 'learning_rate': 0.6438909198174794, 'reg_alpha': 4, 'reg_lambda': 38, 'subsample': 0.8568428760344062}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:07:17,468]\u001b[0m Trial 51 finished with value: -1.1464974229562301 and parameters: {'n_estimators': 42, 'colsample_bytree': 0.8566127107518468, 'max_depth': 4, 'learning_rate': 0.654556703632702, 'reg_alpha': 4, 'reg_lambda': 38, 'subsample': 0.8665891688664465}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:08:48,923]\u001b[0m Trial 52 finished with value: -1.1472164681293031 and parameters: {'n_estimators': 42, 'colsample_bytree': 0.9218840653755758, 'max_depth': 4, 'learning_rate': 0.6410239408697989, 'reg_alpha': 4, 'reg_lambda': 39, 'subsample': 0.8536400717639013}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:09:57,308]\u001b[0m Trial 53 finished with value: -1.1486044262575972 and parameters: {'n_estimators': 42, 'colsample_bytree': 0.9286997679465221, 'max_depth': 4, 'learning_rate': 0.6484032570961211, 'reg_alpha': 4, 'reg_lambda': 37, 'subsample': 0.8462199215805827}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:10:52,972]\u001b[0m Trial 54 finished with value: -1.168780448645645 and parameters: {'n_estimators': 40, 'colsample_bytree': 0.8478868555536415, 'max_depth': 4, 'learning_rate': 0.6990049396685569, 'reg_alpha': 17, 'reg_lambda': 21, 'subsample': 0.8707645213789503}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:11:38,709]\u001b[0m Trial 55 finished with value: -1.1470693847946238 and parameters: {'n_estimators': 38, 'colsample_bytree': 0.9972757024851953, 'max_depth': 3, 'learning_rate': 0.8393162145510874, 'reg_alpha': 0, 'reg_lambda': 39, 'subsample': 0.8926752337451048}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:12:20,731]\u001b[0m Trial 56 finished with value: -1.1494795395980262 and parameters: {'n_estimators': 35, 'colsample_bytree': 0.9985768642912902, 'max_depth': 3, 'learning_rate': 0.8825542061320345, 'reg_alpha': 0, 'reg_lambda': 43, 'subsample': 0.9153821000558096}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:13:10,623]\u001b[0m Trial 57 finished with value: -1.144999804630358 and parameters: {'n_estimators': 40, 'colsample_bytree': 0.9897616970917757, 'max_depth': 3, 'learning_rate': 0.825870855984578, 'reg_alpha': 0, 'reg_lambda': 27, 'subsample': 0.8953311438336251}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:14:41,945]\u001b[0m Trial 58 finished with value: -1.1887879979262683 and parameters: {'n_estimators': 40, 'colsample_bytree': 0.9674962325471664, 'max_depth': 5, 'learning_rate': 0.831284829967749, 'reg_alpha': 5, 'reg_lambda': 26, 'subsample': 0.7963555841454505}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:15:16,938]\u001b[0m Trial 59 finished with value: -1.2237963456152101 and parameters: {'n_estimators': 44, 'colsample_bytree': 0.8998797116263698, 'max_depth': 3, 'learning_rate': 0.5826630742566936, 'reg_alpha': 9, 'reg_lambda': 32, 'subsample': 0.20829439136375588}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:15:41,882]\u001b[0m Trial 60 finished with value: -1.2228842882217634 and parameters: {'n_estimators': 46, 'colsample_bytree': 0.20475732766924765, 'max_depth': 2, 'learning_rate': 0.7753375757586368, 'reg_alpha': 15, 'reg_lambda': 15, 'subsample': 0.9593356297255528}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:16:26,751]\u001b[0m Trial 61 finished with value: -1.1480784142759737 and parameters: {'n_estimators': 38, 'colsample_bytree': 0.9909439087756521, 'max_depth': 3, 'learning_rate': 0.8378540051144937, 'reg_alpha': 0, 'reg_lambda': 50, 'subsample': 0.881681352107341}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:17:21,176]\u001b[0m Trial 62 finished with value: -1.1540224821150669 and parameters: {'n_estimators': 37, 'colsample_bytree': 0.8788139726956327, 'max_depth': 4, 'learning_rate': 0.7814768962173504, 'reg_alpha': 0, 'reg_lambda': 30, 'subsample': 0.8859363063522517}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:18:08,155]\u001b[0m Trial 63 finished with value: -1.1606800413452865 and parameters: {'n_estimators': 40, 'colsample_bytree': 0.9524110052645304, 'max_depth': 3, 'learning_rate': 0.4889522749202039, 'reg_alpha': 7, 'reg_lambda': 39, 'subsample': 0.9069023983770669}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:19:09,473]\u001b[0m Trial 64 finished with value: -1.1469548942659094 and parameters: {'n_estimators': 34, 'colsample_bytree': 0.8581182413709778, 'max_depth': 5, 'learning_rate': 0.6114015541876879, 'reg_alpha': 4, 'reg_lambda': 26, 'subsample': 0.9194500559596277}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:20:03,199]\u001b[0m Trial 65 finished with value: -1.1475249961925917 and parameters: {'n_estimators': 30, 'colsample_bytree': 0.8021217578091362, 'max_depth': 5, 'learning_rate': 0.6155873038925646, 'reg_alpha': 3, 'reg_lambda': 25, 'subsample': 0.9219365757301613}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:21:04,113]\u001b[0m Trial 66 finished with value: -1.1652995483411965 and parameters: {'n_estimators': 34, 'colsample_bytree': 0.8631909640112829, 'max_depth': 5, 'learning_rate': 0.697487441539169, 'reg_alpha': 8, 'reg_lambda': 35, 'subsample': 0.8021065575749745}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:22:03,157]\u001b[0m Trial 67 finished with value: -1.1733003144357297 and parameters: {'n_estimators': 43, 'colsample_bytree': 0.8125797411404332, 'max_depth': 4, 'learning_rate': 0.532813899724177, 'reg_alpha': 19, 'reg_lambda': 21, 'subsample': 0.8322416239821291}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:23:20,669]\u001b[0m Trial 68 finished with value: -1.1666600889933774 and parameters: {'n_estimators': 47, 'colsample_bytree': 0.8542360376849204, 'max_depth': 5, 'learning_rate': 0.5884009114129404, 'reg_alpha': 14, 'reg_lambda': 6, 'subsample': 0.7170968055061562}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:24:16,897]\u001b[0m Trial 69 finished with value: -1.191699060197517 and parameters: {'n_estimators': 32, 'colsample_bytree': 0.9538085411680626, 'max_depth': 5, 'learning_rate': 0.6575157112709412, 'reg_alpha': 24, 'reg_lambda': 15, 'subsample': 0.7714489982105232}. Best is trial 49 with value: -1.1427021471671321.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:25:26,121]\u001b[0m Trial 70 finished with value: -1.1399100487904021 and parameters: {'n_estimators': 44, 'colsample_bytree': 0.8891477452551094, 'max_depth': 4, 'learning_rate': 0.48526425007781543, 'reg_alpha': 3, 'reg_lambda': 45, 'subsample': 0.862782883647167}. Best is trial 70 with value: -1.1399100487904021.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:26:32,101]\u001b[0m Trial 71 finished with value: -1.1420826342868629 and parameters: {'n_estimators': 44, 'colsample_bytree': 0.8677708655608936, 'max_depth': 4, 'learning_rate': 0.532454088054604, 'reg_alpha': 6, 'reg_lambda': 46, 'subsample': 0.9517025178443523}. Best is trial 70 with value: -1.1399100487904021.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:27:40,586]\u001b[0m Trial 72 finished with value: -1.1559469434603984 and parameters: {'n_estimators': 44, 'colsample_bytree': 0.9007546534782491, 'max_depth': 4, 'learning_rate': 0.42659867765454484, 'reg_alpha': 10, 'reg_lambda': 44, 'subsample': 0.8505710722180004}. Best is trial 70 with value: -1.1399100487904021.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:28:45,746]\u001b[0m Trial 73 finished with value: -1.1493488878425613 and parameters: {'n_estimators': 41, 'colsample_bytree': 0.8820059142995933, 'max_depth': 4, 'learning_rate': 0.47561385200238987, 'reg_alpha': 6, 'reg_lambda': 49, 'subsample': 0.8677710830030801}. Best is trial 70 with value: -1.1399100487904021.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:29:58,580]\u001b[0m Trial 74 finished with value: -1.1368676654457603 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.7677804691140988, 'max_depth': 4, 'learning_rate': 0.5066687862813439, 'reg_alpha': 2, 'reg_lambda': 52, 'subsample': 0.9407381484384736}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:30:54,616]\u001b[0m Trial 75 finished with value: -1.142270794572612 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.7604361083772712, 'max_depth': 3, 'learning_rate': 0.5136775107785333, 'reg_alpha': 2, 'reg_lambda': 47, 'subsample': 0.9552031583591213}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:31:45,562]\u001b[0m Trial 76 finished with value: -1.1413411274599343 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.7416841378458358, 'max_depth': 3, 'learning_rate': 0.5141622410000932, 'reg_alpha': 0, 'reg_lambda': 52, 'subsample': 0.948985003926204}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:32:24,288]\u001b[0m Trial 77 finished with value: -1.1746421146550086 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.7615944198430566, 'max_depth': 2, 'learning_rate': 0.5164598625074056, 'reg_alpha': 1, 'reg_lambda': 52, 'subsample': 0.9452888555231685}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:33:20,561]\u001b[0m Trial 78 finished with value: -1.1577372533238253 and parameters: {'n_estimators': 49, 'colsample_bytree': 0.7753276055574895, 'max_depth': 3, 'learning_rate': 0.5487791641911057, 'reg_alpha': 9, 'reg_lambda': 59, 'subsample': 0.9998494584112011}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:34:02,506]\u001b[0m Trial 79 finished with value: -1.293096688904562 and parameters: {'n_estimators': 47, 'colsample_bytree': 0.8113057695326291, 'max_depth': 2, 'learning_rate': 0.4289868315827875, 'reg_alpha': 55, 'reg_lambda': 53, 'subsample': 0.9625465140166196}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:34:54,269]\u001b[0m Trial 80 finished with value: -1.1654036160256025 and parameters: {'n_estimators': 50, 'colsample_bytree': 0.6825988954169676, 'max_depth': 3, 'learning_rate': 0.5052368567694161, 'reg_alpha': 12, 'reg_lambda': 42, 'subsample': 0.9618038509371534}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:35:41,167]\u001b[0m Trial 81 finished with value: -1.1520399538141697 and parameters: {'n_estimators': 45, 'colsample_bytree': 0.7353807231298576, 'max_depth': 3, 'learning_rate': 0.4687118402388607, 'reg_alpha': 2, 'reg_lambda': 45, 'subsample': 0.9024502679283466}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:36:30,900]\u001b[0m Trial 82 finished with value: -1.161376799419333 and parameters: {'n_estimators': 48, 'colsample_bytree': 0.7120043804919223, 'max_depth': 3, 'learning_rate': 0.44497817291782266, 'reg_alpha': 7, 'reg_lambda': 48, 'subsample': 0.9371715837835035}. Best is trial 74 with value: -1.1368676654457603.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:37:41,719]\u001b[0m Trial 83 finished with value: -1.1320039352225888 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.739738307941033, 'max_depth': 4, 'learning_rate': 0.5460400624111504, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.9773201760674207}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:38:39,328]\u001b[0m Trial 84 finished with value: -1.1362526980450478 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.4723972347230013, 'max_depth': 4, 'learning_rate': 0.5443694106842633, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9993536855402133}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:39:36,403]\u001b[0m Trial 85 finished with value: -1.1395793865464339 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.47157783821357246, 'max_depth': 4, 'learning_rate': 0.5462913903234429, 'reg_alpha': 2, 'reg_lambda': 54, 'subsample': 0.9857364941579655}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:40:31,584]\u001b[0m Trial 86 finished with value: -1.1464377541092263 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.42475183128162264, 'max_depth': 4, 'learning_rate': 0.5420098347625687, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9802591806858411}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:41:22,271]\u001b[0m Trial 87 finished with value: -1.1616054953422075 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.3162072712632286, 'max_depth': 4, 'learning_rate': 0.577636454923968, 'reg_alpha': 7, 'reg_lambda': 55, 'subsample': 0.9927459826241929}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:42:17,659]\u001b[0m Trial 88 finished with value: -1.1562592515181087 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.4946952515556816, 'max_depth': 4, 'learning_rate': 0.5652673437777521, 'reg_alpha': 10, 'reg_lambda': 61, 'subsample': 0.9981821795405251}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:43:13,026]\u001b[0m Trial 89 finished with value: -1.1442516559751947 and parameters: {'n_estimators': 50, 'colsample_bytree': 0.4360149585470833, 'max_depth': 4, 'learning_rate': 0.521379957979783, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.9526945069376993}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:44:07,378]\u001b[0m Trial 90 finished with value: -1.1784167159650438 and parameters: {'n_estimators': 47, 'colsample_bytree': 0.3766221271598966, 'max_depth': 4, 'learning_rate': 0.4857192506004895, 'reg_alpha': 13, 'reg_lambda': 52, 'subsample': 0.9790029768954502}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:45:04,850]\u001b[0m Trial 91 finished with value: -1.138836275587506 and parameters: {'n_estimators': 50, 'colsample_bytree': 0.4547933006492024, 'max_depth': 4, 'learning_rate': 0.5227877407058623, 'reg_alpha': 2, 'reg_lambda': 46, 'subsample': 0.9539705524332679}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:46:12,186]\u001b[0m Trial 92 finished with value: -1.1370334063720309 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5534576505305613, 'max_depth': 4, 'learning_rate': 0.6059210538829022, 'reg_alpha': 2, 'reg_lambda': 47, 'subsample': 0.933019197988232}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:46:59,611]\u001b[0m Trial 93 finished with value: -1.1535688753298134 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.4731666724753366, 'max_depth': 3, 'learning_rate': 0.6041840263504948, 'reg_alpha': 6, 'reg_lambda': 47, 'subsample': 0.9401337773233223}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:48:02,382]\u001b[0m Trial 94 finished with value: -1.1335531590452543 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5358830413334975, 'max_depth': 4, 'learning_rate': 0.5608542081579281, 'reg_alpha': 0, 'reg_lambda': 46, 'subsample': 0.9757123823231674}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:49:06,341]\u001b[0m Trial 95 finished with value: -1.1386977771551774 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5545452048243751, 'max_depth': 4, 'learning_rate': 0.5445477694698863, 'reg_alpha': 2, 'reg_lambda': 57, 'subsample': 0.9273351185407479}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:50:12,956]\u001b[0m Trial 96 finished with value: -1.1388207739524305 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5310470473460771, 'max_depth': 4, 'learning_rate': 0.561409357952515, 'reg_alpha': 2, 'reg_lambda': 57, 'subsample': 0.928335053717267}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:51:16,382]\u001b[0m Trial 97 finished with value: -1.1521984579600557 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5534938693851502, 'max_depth': 4, 'learning_rate': 0.5638814793120619, 'reg_alpha': 9, 'reg_lambda': 57, 'subsample': 0.9295973392353116}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:52:25,351]\u001b[0m Trial 98 finished with value: -1.1384322264839053 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5383154983365184, 'max_depth': 4, 'learning_rate': 0.6284441173555269, 'reg_alpha': 2, 'reg_lambda': 67, 'subsample': 0.9795775173283272}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:53:47,746]\u001b[0m Trial 99 finished with value: -1.14402672932352 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5271741091075497, 'max_depth': 5, 'learning_rate': 0.6253913426031796, 'reg_alpha': 2, 'reg_lambda': 67, 'subsample': 0.9761366674199372}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:54:51,188]\u001b[0m Trial 100 finished with value: -1.1496528569727127 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5728442125461837, 'max_depth': 4, 'learning_rate': 0.6822976449026291, 'reg_alpha': 11, 'reg_lambda': 62, 'subsample': 0.9727953965599395}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:55:55,346]\u001b[0m Trial 101 finished with value: -1.139952951367138 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5035230345315796, 'max_depth': 4, 'learning_rate': 0.5975084827884041, 'reg_alpha': 2, 'reg_lambda': 64, 'subsample': 0.9269998657797838}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:56:56,558]\u001b[0m Trial 102 finished with value: -1.132892362443838 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.6156809096180901, 'max_depth': 4, 'learning_rate': 0.5668169512872326, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.999129740718747}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:57:34,219]\u001b[0m Trial 103 finished with value: -1.3230593998500986 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.5489885152560696, 'max_depth': 4, 'learning_rate': 0.5706099828638901, 'reg_alpha': 90, 'reg_lambda': 57, 'subsample': 0.9939210944978598}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:58:36,020]\u001b[0m Trial 104 finished with value: -1.171814744842226 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5953398131680022, 'max_depth': 5, 'learning_rate': 0.5470997432942872, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.4130725351914253}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 02:59:38,615]\u001b[0m Trial 105 finished with value: -1.1496045070667815 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.48491947508424504, 'max_depth': 4, 'learning_rate': 0.6725756153074279, 'reg_alpha': 6, 'reg_lambda': 50, 'subsample': 0.9128818183763782}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:00:41,753]\u001b[0m Trial 106 finished with value: -1.1359670830014306 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.4516306510142738, 'max_depth': 4, 'learning_rate': 0.6310228596593175, 'reg_alpha': 0, 'reg_lambda': 54, 'subsample': 0.999602576692762}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:01:49,214]\u001b[0m Trial 107 finished with value: -1.1512017526468634 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.4479628980948187, 'max_depth': 4, 'learning_rate': 0.627494707740201, 'reg_alpha': 8, 'reg_lambda': 69, 'subsample': 0.9659198707612308}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:03:03,241]\u001b[0m Trial 108 finished with value: -1.1326746447976994 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5724690251325802, 'max_depth': 4, 'learning_rate': 0.5947145051778949, 'reg_alpha': 0, 'reg_lambda': 42, 'subsample': 0.9287881349193077}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:04:46,864]\u001b[0m Trial 109 finished with value: -1.1425985557626646 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.6072239796295207, 'max_depth': 5, 'learning_rate': 0.5934889929795117, 'reg_alpha': 5, 'reg_lambda': 65, 'subsample': 0.9998768261348437}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:06:00,573]\u001b[0m Trial 110 finished with value: -1.1367517928555761 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.570374260476805, 'max_depth': 4, 'learning_rate': 0.6334304788365611, 'reg_alpha': 0, 'reg_lambda': 41, 'subsample': 0.9109367825573227}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:07:14,925]\u001b[0m Trial 111 finished with value: -1.1361352111026979 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.5681699911589904, 'max_depth': 4, 'learning_rate': 0.6240290335320327, 'reg_alpha': 0, 'reg_lambda': 42, 'subsample': 0.9307630827496381}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:08:24,744]\u001b[0m Trial 112 finished with value: -1.1415362452494437 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.572111606548745, 'max_depth': 4, 'learning_rate': 0.6293298412953813, 'reg_alpha': 5, 'reg_lambda': 41, 'subsample': 0.903302510563128}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:09:52,716]\u001b[0m Trial 113 finished with value: -1.1680455600845434 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6335311120178663, 'max_depth': 5, 'learning_rate': 0.7083694085547037, 'reg_alpha': 0, 'reg_lambda': 36, 'subsample': 0.9775013318977399}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:10:56,280]\u001b[0m Trial 114 finished with value: -1.136356187706804 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5116753490325009, 'max_depth': 4, 'learning_rate': 0.668286605224078, 'reg_alpha': 0, 'reg_lambda': 50, 'subsample': 0.9984625552275582}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:11:36,270]\u001b[0m Trial 115 finished with value: -1.2896268515344391 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.5140654699026259, 'max_depth': 4, 'learning_rate': 0.6842367478757202, 'reg_alpha': 71, 'reg_lambda': 50, 'subsample': 0.9975897812492408}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:12:40,794]\u001b[0m Trial 116 finished with value: -1.1352244191500325 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.589379102007903, 'max_depth': 4, 'learning_rate': 0.6618048543534676, 'reg_alpha': 0, 'reg_lambda': 42, 'subsample': 0.9667041749982004}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:13:39,334]\u001b[0m Trial 117 finished with value: -1.1334623019893995 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.584202769595485, 'max_depth': 3, 'learning_rate': 0.7255523583660942, 'reg_alpha': 0, 'reg_lambda': 42, 'subsample': 0.9636131382597459}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:14:27,526]\u001b[0m Trial 118 finished with value: -1.1644029714924353 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.6589730584962741, 'max_depth': 3, 'learning_rate': 0.7390431203997543, 'reg_alpha': 0, 'reg_lambda': 33, 'subsample': 0.49385182672793015}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:16:00,185]\u001b[0m Trial 119 finished with value: -1.159750801397407 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.5891879814222091, 'max_depth': 5, 'learning_rate': 0.7165386877594715, 'reg_alpha': 0, 'reg_lambda': 43, 'subsample': 0.9999894063833084}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:16:48,978]\u001b[0m Trial 120 finished with value: -1.1500305285200507 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.41010963118685495, 'max_depth': 3, 'learning_rate': 0.6575052740124974, 'reg_alpha': 5, 'reg_lambda': 42, 'subsample': 0.9598214574503265}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:17:52,410]\u001b[0m Trial 121 finished with value: -1.138139918552817 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5663200252310384, 'max_depth': 4, 'learning_rate': 0.6637868240434669, 'reg_alpha': 0, 'reg_lambda': 51, 'subsample': 0.9374610216419422}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:18:59,190]\u001b[0m Trial 122 finished with value: -1.1388305621828354 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6163426762418752, 'max_depth': 4, 'learning_rate': 0.5923031739980139, 'reg_alpha': 4, 'reg_lambda': 35, 'subsample': 0.9074192610247116}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:19:41,288]\u001b[0m Trial 123 finished with value: -1.2389298299406983 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.6299486848808431, 'max_depth': 4, 'learning_rate': 0.7334522278273621, 'reg_alpha': 47, 'reg_lambda': 48, 'subsample': 0.968109738221919}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:20:53,215]\u001b[0m Trial 124 finished with value: -1.1588742827549172 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.5850868878645428, 'max_depth': 4, 'learning_rate': 0.7747259030271882, 'reg_alpha': 0, 'reg_lambda': 44, 'subsample': 0.8834567226946171}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:21:41,135]\u001b[0m Trial 125 finished with value: -1.1530136695776432 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5403816818299925, 'max_depth': 3, 'learning_rate': 0.6135171758929646, 'reg_alpha': 8, 'reg_lambda': 41, 'subsample': 0.9405307293370506}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:22:58,141]\u001b[0m Trial 126 finished with value: -1.1432619005903795 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5095126387721921, 'max_depth': 5, 'learning_rate': 0.5809475598790659, 'reg_alpha': 3, 'reg_lambda': 53, 'subsample': 0.9858861205573737}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:24:06,292]\u001b[0m Trial 127 finished with value: -1.1375521847800378 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.6071549307575325, 'max_depth': 4, 'learning_rate': 0.6415995097610783, 'reg_alpha': 0, 'reg_lambda': 45, 'subsample': 0.9169092289891425}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:24:46,742]\u001b[0m Trial 128 finished with value: -1.3432119857556297 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.561509505105169, 'max_depth': 4, 'learning_rate': 0.6978135366646672, 'reg_alpha': 100, 'reg_lambda': 48, 'subsample': 0.9664671553293698}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:25:33,934]\u001b[0m Trial 129 finished with value: -1.154905587682633 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.4920012420895217, 'max_depth': 3, 'learning_rate': 0.6070740219287424, 'reg_alpha': 6, 'reg_lambda': 50, 'subsample': 0.9474085470828405}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:26:51,839]\u001b[0m Trial 130 finished with value: -1.141286284815402 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.6525131755564769, 'max_depth': 4, 'learning_rate': 0.6711739242787046, 'reg_alpha': 4, 'reg_lambda': 40, 'subsample': 0.9964002387888141}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:27:53,377]\u001b[0m Trial 131 finished with value: -1.1367129225277237 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.6162326220895982, 'max_depth': 4, 'learning_rate': 0.6463984584890012, 'reg_alpha': 0, 'reg_lambda': 44, 'subsample': 0.9112090163161126}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:28:56,854]\u001b[0m Trial 132 finished with value: -1.14005020053481 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.579704022977021, 'max_depth': 4, 'learning_rate': 0.6401377753759981, 'reg_alpha': 3, 'reg_lambda': 38, 'subsample': 0.8968875170549507}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:29:59,041]\u001b[0m Trial 133 finished with value: -1.1481139163478726 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.6235507654245267, 'max_depth': 4, 'learning_rate': 0.6830432527985993, 'reg_alpha': 7, 'reg_lambda': 43, 'subsample': 0.9188076783362029}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:31:04,521]\u001b[0m Trial 134 finished with value: -1.136285890950123 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5980618180235607, 'max_depth': 4, 'learning_rate': 0.5730608857545839, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.8801988160166143}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:32:06,929]\u001b[0m Trial 135 finished with value: -1.1327735857427554 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5973903976008476, 'max_depth': 4, 'learning_rate': 0.577280262170504, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9650092608599018}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:33:18,754]\u001b[0m Trial 136 finished with value: -1.1513979689084106 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.6418136372541108, 'max_depth': 4, 'learning_rate': 0.754533803964909, 'reg_alpha': 5, 'reg_lambda': 59, 'subsample': 0.8846551706073764}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:34:41,416]\u001b[0m Trial 137 finished with value: -1.1394214944884378 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.6003873710478098, 'max_depth': 5, 'learning_rate': 0.5738984467808348, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.9663529932529569}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:35:21,477]\u001b[0m Trial 138 finished with value: -1.1611168896655537 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.5228161796616738, 'max_depth': 2, 'learning_rate': 0.6453796734054589, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.9994555320886886}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:36:13,037]\u001b[0m Trial 139 finished with value: -1.1376442222308103 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.6136149244501622, 'max_depth': 3, 'learning_rate': 0.6261647686459686, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9828973897954258}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:37:21,969]\u001b[0m Trial 140 finished with value: -1.1475645989630836 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.5685431925384956, 'max_depth': 4, 'learning_rate': 0.5977858938305944, 'reg_alpha': 9, 'reg_lambda': 40, 'subsample': 0.9518033810795088}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:38:37,682]\u001b[0m Trial 141 finished with value: -1.136859222969619 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5904909584257237, 'max_depth': 4, 'learning_rate': 0.5602581044669311, 'reg_alpha': 3, 'reg_lambda': 53, 'subsample': 0.9314233196158255}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:39:43,895]\u001b[0m Trial 142 finished with value: -1.133322528883053 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5894244318333233, 'max_depth': 4, 'learning_rate': 0.5621155463306277, 'reg_alpha': 0, 'reg_lambda': 54, 'subsample': 0.9112684901107343}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:40:48,223]\u001b[0m Trial 143 finished with value: -1.134508150741842 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.590525215036294, 'max_depth': 4, 'learning_rate': 0.5875970777576565, 'reg_alpha': 0, 'reg_lambda': 59, 'subsample': 0.9142832332695064}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:41:34,275]\u001b[0m Trial 144 finished with value: -1.1838717575813757 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.4686262652302952, 'max_depth': 4, 'learning_rate': 0.5822489130071635, 'reg_alpha': 0, 'reg_lambda': 59, 'subsample': 0.3034394584729186}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:42:36,141]\u001b[0m Trial 145 finished with value: -1.1446108681543068 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5423220523379737, 'max_depth': 4, 'learning_rate': 0.5279061070412316, 'reg_alpha': 4, 'reg_lambda': 63, 'subsample': 0.8973039941479423}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:44:02,514]\u001b[0m Trial 146 finished with value: -1.1454201027772417 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.6515415992861553, 'max_depth': 5, 'learning_rate': 0.6140897731742174, 'reg_alpha': 2, 'reg_lambda': 56, 'subsample': 0.9605438509833749}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:45:01,658]\u001b[0m Trial 147 finished with value: -1.1450656361434444 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.5991413406350417, 'max_depth': 4, 'learning_rate': 0.5551140424561292, 'reg_alpha': 6, 'reg_lambda': 60, 'subsample': 0.9808997591842122}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:46:01,967]\u001b[0m Trial 148 finished with value: -1.1348665033950547 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5831372082361163, 'max_depth': 4, 'learning_rate': 0.5724909315578404, 'reg_alpha': 0, 'reg_lambda': 54, 'subsample': 0.875089497299342}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:46:49,994]\u001b[0m Trial 149 finished with value: -1.1529218958974188 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.40543376220340793, 'max_depth': 3, 'learning_rate': 0.5376465542967032, 'reg_alpha': 2, 'reg_lambda': 54, 'subsample': 0.8296478826387635}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:47:54,083]\u001b[0m Trial 150 finished with value: -1.1392275378726184 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.670610835604207, 'max_depth': 4, 'learning_rate': 0.5779846577576295, 'reg_alpha': 4, 'reg_lambda': 57, 'subsample': 0.9506217403618551}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:48:53,349]\u001b[0m Trial 151 finished with value: -1.1367667861988542 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.5860274800509163, 'max_depth': 4, 'learning_rate': 0.5880939217044168, 'reg_alpha': 0, 'reg_lambda': 50, 'subsample': 0.8748428789101284}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:49:57,221]\u001b[0m Trial 152 finished with value: -1.1338960478510054 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.6123972354166497, 'max_depth': 4, 'learning_rate': 0.5575676505300048, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.9202867994141823}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:51:06,477]\u001b[0m Trial 153 finished with value: -1.1381762258750339 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.6280567014645441, 'max_depth': 4, 'learning_rate': 0.5591371280353937, 'reg_alpha': 2, 'reg_lambda': 61, 'subsample': 0.8932376653965283}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:52:09,665]\u001b[0m Trial 154 finished with value: -1.134950669235322 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5535197151164615, 'max_depth': 4, 'learning_rate': 0.602765737659617, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9997483973998662}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:53:21,059]\u001b[0m Trial 155 finished with value: -1.139652944546273 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.5788284270951625, 'max_depth': 4, 'learning_rate': 0.5692295323171224, 'reg_alpha': 4, 'reg_lambda': 70, 'subsample': 0.9265916045646915}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:54:25,243]\u001b[0m Trial 156 finished with value: -1.1442182079724144 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5596118703763371, 'max_depth': 4, 'learning_rate': 0.6018633769826036, 'reg_alpha': 6, 'reg_lambda': 65, 'subsample': 0.9720887848744588}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:55:24,422]\u001b[0m Trial 157 finished with value: -1.2025996899791394 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.6026049009848683, 'max_depth': 5, 'learning_rate': 0.5442858257058324, 'reg_alpha': 34, 'reg_lambda': 58, 'subsample': 0.9415709750579325}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:56:27,936]\u001b[0m Trial 158 finished with value: -1.1397118453504624 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5352417156435245, 'max_depth': 4, 'learning_rate': 0.5897040073734231, 'reg_alpha': 2, 'reg_lambda': 63, 'subsample': 0.9170174966391416}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:57:15,664]\u001b[0m Trial 159 finished with value: -1.1501787675832786 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5531443607587038, 'max_depth': 3, 'learning_rate': 0.614558153862805, 'reg_alpha': 0, 'reg_lambda': 68, 'subsample': 0.6825282278556518}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:58:16,598]\u001b[0m Trial 160 finished with value: -1.1500887148379362 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.640333447461556, 'max_depth': 4, 'learning_rate': 0.5662120727396689, 'reg_alpha': 8, 'reg_lambda': 61, 'subsample': 0.8779513885636006}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 03:59:18,698]\u001b[0m Trial 161 finished with value: -1.1362044280950143 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5260040970255199, 'max_depth': 4, 'learning_rate': 0.6207055782288287, 'reg_alpha': 0, 'reg_lambda': 73, 'subsample': 0.9989537786218593}. Best is trial 83 with value: -1.1320039352225888.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:00:22,380]\u001b[0m Trial 162 finished with value: -1.130799403316559 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5809380542790228, 'max_depth': 4, 'learning_rate': 0.5997502684732103, 'reg_alpha': 0, 'reg_lambda': 56, 'subsample': 0.9873836998847414}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:01:27,409]\u001b[0m Trial 163 finished with value: -1.136555822017335 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.576705197088663, 'max_depth': 4, 'learning_rate': 0.6190440699817685, 'reg_alpha': 3, 'reg_lambda': 56, 'subsample': 0.9996492502527348}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:02:33,666]\u001b[0m Trial 164 finished with value: -1.1379198654477598 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.528306961851815, 'max_depth': 4, 'learning_rate': 0.4994026516735154, 'reg_alpha': 2, 'reg_lambda': 75, 'subsample': 0.9865311653910167}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:03:45,001]\u001b[0m Trial 165 finished with value: -1.134438957357105 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5421461782273269, 'max_depth': 4, 'learning_rate': 0.5342943531529285, 'reg_alpha': 0, 'reg_lambda': 72, 'subsample': 0.9730507681045041}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:04:52,311]\u001b[0m Trial 166 finished with value: -1.143232949487765 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.5435828604564837, 'max_depth': 4, 'learning_rate': 0.5969016764924913, 'reg_alpha': 5, 'reg_lambda': 74, 'subsample': 0.9677365210577359}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:05:58,285]\u001b[0m Trial 167 finished with value: -1.1320358974624056 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5630789292350905, 'max_depth': 4, 'learning_rate': 0.5207296162204005, 'reg_alpha': 0, 'reg_lambda': 80, 'subsample': 0.9616696803704091}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:07:21,724]\u001b[0m Trial 168 finished with value: -1.138880093693079 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5648090651667806, 'max_depth': 5, 'learning_rate': 0.5272043193083875, 'reg_alpha': 2, 'reg_lambda': 82, 'subsample': 0.9534338833331282}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:08:36,653]\u001b[0m Trial 169 finished with value: -1.1384430622439805 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.5866004622261715, 'max_depth': 4, 'learning_rate': 0.5068532494951535, 'reg_alpha': 4, 'reg_lambda': 79, 'subsample': 0.9393412188169172}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:10:15,473]\u001b[0m Trial 170 finished with value: -1.1552705364844578 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6134140888994779, 'max_depth': 6, 'learning_rate': 0.5516742307220202, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.970799915641913}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:11:17,719]\u001b[0m Trial 171 finished with value: -1.1346393787603448 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5493374833957387, 'max_depth': 4, 'learning_rate': 0.5806356044473119, 'reg_alpha': 0, 'reg_lambda': 72, 'subsample': 0.9801998939786122}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:12:19,522]\u001b[0m Trial 172 finished with value: -1.1386387740685064 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5548393402885977, 'max_depth': 4, 'learning_rate': 0.525389422569236, 'reg_alpha': 2, 'reg_lambda': 71, 'subsample': 0.9569935875188909}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:13:30,555]\u001b[0m Trial 173 finished with value: -1.13384750615541 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5805474921516898, 'max_depth': 4, 'learning_rate': 0.5795131138713703, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9779613525500364}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:14:37,294]\u001b[0m Trial 174 finished with value: -1.133044735566466 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5858843166922822, 'max_depth': 4, 'learning_rate': 0.5793828096869968, 'reg_alpha': 0, 'reg_lambda': 68, 'subsample': 0.9809685331225042}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:15:44,029]\u001b[0m Trial 175 finished with value: -1.1359799187119723 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5860621230147405, 'max_depth': 4, 'learning_rate': 0.5807949378395256, 'reg_alpha': 3, 'reg_lambda': 66, 'subsample': 0.9808084204642339}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:16:53,559]\u001b[0m Trial 176 finished with value: -1.141473587553517 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.6033438227677306, 'max_depth': 4, 'learning_rate': 0.5376007057742077, 'reg_alpha': 6, 'reg_lambda': 71, 'subsample': 0.9816427763861362}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:17:41,055]\u001b[0m Trial 177 finished with value: -1.1444469258920464 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5485848072225864, 'max_depth': 3, 'learning_rate': 0.5660550333635197, 'reg_alpha': 0, 'reg_lambda': 69, 'subsample': 0.9651219308285371}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:18:44,915]\u001b[0m Trial 178 finished with value: -1.137020506637327 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5754839139133451, 'max_depth': 4, 'learning_rate': 0.5579260310235525, 'reg_alpha': 2, 'reg_lambda': 65, 'subsample': 0.9494315544315293}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:19:53,513]\u001b[0m Trial 179 finished with value: -1.138354379118715 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.6276174585430218, 'max_depth': 4, 'learning_rate': 0.5864810091833345, 'reg_alpha': 4, 'reg_lambda': 77, 'subsample': 0.9837683877300665}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:20:56,977]\u001b[0m Trial 180 finished with value: -1.1328209499892734 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5917816657094719, 'max_depth': 4, 'learning_rate': 0.47523704373231207, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.965521825791473}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:22:00,484]\u001b[0m Trial 181 finished with value: -1.133847866538866 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5873826935295289, 'max_depth': 4, 'learning_rate': 0.4795325565554045, 'reg_alpha': 0, 'reg_lambda': 68, 'subsample': 0.970802757784503}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:23:00,703]\u001b[0m Trial 182 finished with value: -1.1372064494164618 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5579921079308136, 'max_depth': 4, 'learning_rate': 0.46305841115080465, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.9423921489022218}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:24:12,647]\u001b[0m Trial 183 finished with value: -1.1373700251758931 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.6127684915967512, 'max_depth': 4, 'learning_rate': 0.4918487794834131, 'reg_alpha': 3, 'reg_lambda': 63, 'subsample': 0.9841246083231099}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:25:21,306]\u001b[0m Trial 184 finished with value: -1.132905984077819 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5734798668487057, 'max_depth': 4, 'learning_rate': 0.48031841423141963, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9540965633934726}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:26:23,914]\u001b[0m Trial 185 finished with value: -1.1375890793796848 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5950669874681914, 'max_depth': 4, 'learning_rate': 0.47335022574757746, 'reg_alpha': 2, 'reg_lambda': 73, 'subsample': 0.9560485775448001}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:27:44,575]\u001b[0m Trial 186 finished with value: -1.137413101839578 and parameters: {'n_estimators': 70, 'colsample_bytree': 0.5796659850562357, 'max_depth': 4, 'learning_rate': 0.4450352430878345, 'reg_alpha': 5, 'reg_lambda': 69, 'subsample': 0.9305436789113651}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:28:45,908]\u001b[0m Trial 187 finished with value: -1.1342408452714952 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5699108401309227, 'max_depth': 4, 'learning_rate': 0.4861755356372107, 'reg_alpha': 0, 'reg_lambda': 71, 'subsample': 0.9680145156613303}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:29:28,794]\u001b[0m Trial 188 finished with value: -1.3048578669656763 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5348233261896181, 'max_depth': 4, 'learning_rate': 0.4947348521235925, 'reg_alpha': 80, 'reg_lambda': 75, 'subsample': 0.9756832761234232}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:30:32,609]\u001b[0m Trial 189 finished with value: -1.1363834075887924 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.5679481894062545, 'max_depth': 4, 'learning_rate': 0.4741788634214712, 'reg_alpha': 2, 'reg_lambda': 72, 'subsample': 0.9638934219782596}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:31:54,939]\u001b[0m Trial 190 finished with value: -1.1429321594943795 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.6204611016465887, 'max_depth': 5, 'learning_rate': 0.4155355969230499, 'reg_alpha': 7, 'reg_lambda': 68, 'subsample': 0.9437027124171402}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:32:55,527]\u001b[0m Trial 191 finished with value: -1.1348048290304251 and parameters: {'n_estimators': 53, 'colsample_bytree': 0.5759215344735396, 'max_depth': 4, 'learning_rate': 0.5108608923557857, 'reg_alpha': 0, 'reg_lambda': 70, 'subsample': 0.9185391129829537}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:34:08,673]\u001b[0m Trial 192 finished with value: -1.1339413961443363 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5968931601196067, 'max_depth': 4, 'learning_rate': 0.5130341476546718, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.9218021833878884}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:35:12,278]\u001b[0m Trial 193 finished with value: -1.1369441481854343 and parameters: {'n_estimators': 55, 'colsample_bytree': 0.6029894872956377, 'max_depth': 4, 'learning_rate': 0.4793719241993558, 'reg_alpha': 2, 'reg_lambda': 67, 'subsample': 0.9563869299490942}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:36:16,804]\u001b[0m Trial 194 finished with value: -1.1310121921933527 and parameters: {'n_estimators': 56, 'colsample_bytree': 0.5958417507150661, 'max_depth': 4, 'learning_rate': 0.5163780144112152, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9712053115044598}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:37:21,158]\u001b[0m Trial 195 finished with value: -1.1401251941701336 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.6332218824191989, 'max_depth': 4, 'learning_rate': 0.45395062937522856, 'reg_alpha': 4, 'reg_lambda': 64, 'subsample': 0.9326478697240836}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:38:21,039]\u001b[0m Trial 196 finished with value: -1.1328269750095952 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.5978378402830631, 'max_depth': 4, 'learning_rate': 0.5147515591595009, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9699877734566462}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:39:22,015]\u001b[0m Trial 197 finished with value: -1.13730510525366 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.6042184705155547, 'max_depth': 4, 'learning_rate': 0.5178861394409977, 'reg_alpha': 2, 'reg_lambda': 67, 'subsample': 0.9704040742165746}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:40:01,358]\u001b[0m Trial 198 finished with value: -1.2661729089143203 and parameters: {'n_estimators': 51, 'colsample_bytree': 0.6171244291482841, 'max_depth': 3, 'learning_rate': 0.5028722254583604, 'reg_alpha': 56, 'reg_lambda': 63, 'subsample': 0.9876763720022136}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:41:00,988]\u001b[0m Trial 199 finished with value: -1.1352260491391384 and parameters: {'n_estimators': 52, 'colsample_bytree': 0.5658066195699668, 'max_depth': 4, 'learning_rate': 0.486867649773291, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9486134250589662}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:42:03,647]\u001b[0m Trial 200 finished with value: -1.1398789690701812 and parameters: {'n_estimators': 54, 'colsample_bytree': 0.5934659299078578, 'max_depth': 4, 'learning_rate': 0.5298977878295046, 'reg_alpha': 4, 'reg_lambda': 69, 'subsample': 0.9626241485871795}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:43:12,406]\u001b[0m Trial 201 finished with value: -1.1313389006710097 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.5928076654334489, 'max_depth': 4, 'learning_rate': 0.5339580299003439, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9993293870935633}. Best is trial 162 with value: -1.130799403316559.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:44:23,251]\u001b[0m Trial 202 finished with value: -1.130699397590487 and parameters: {'n_estimators': 57, 'colsample_bytree': 0.648932304712519, 'max_depth': 4, 'learning_rate': 0.5153147113911307, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9976991269242993}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:45:33,074]\u001b[0m Trial 203 finished with value: -1.1338823036728254 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6605490108545999, 'max_depth': 4, 'learning_rate': 0.463162658155724, 'reg_alpha': 2, 'reg_lambda': 65, 'subsample': 0.9964982029207545}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:46:42,946]\u001b[0m Trial 204 finished with value: -1.1331958777553865 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6477281840972874, 'max_depth': 4, 'learning_rate': 0.4576991215567853, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.9974854321293256}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:47:52,414]\u001b[0m Trial 205 finished with value: -1.1331728571420738 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6604949936616951, 'max_depth': 4, 'learning_rate': 0.4378883816434026, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.9974442853160613}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:49:01,868]\u001b[0m Trial 206 finished with value: -1.139259389160879 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6864023658811796, 'max_depth': 4, 'learning_rate': 0.42987594748366165, 'reg_alpha': 5, 'reg_lambda': 64, 'subsample': 0.9962125797263823}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:50:13,949]\u001b[0m Trial 207 finished with value: -1.134195840306166 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.648630292702336, 'max_depth': 4, 'learning_rate': 0.392466050907307, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.9985377918501958}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:51:23,922]\u001b[0m Trial 208 finished with value: -1.1362679675059202 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6686841496230495, 'max_depth': 4, 'learning_rate': 0.46014001397322757, 'reg_alpha': 3, 'reg_lambda': 65, 'subsample': 0.997587687641437}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:52:34,684]\u001b[0m Trial 209 finished with value: -1.1322556336652256 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.650987063848399, 'max_depth': 4, 'learning_rate': 0.43609104658725917, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.9882253778128459}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:53:52,160]\u001b[0m Trial 210 finished with value: -1.1407306094164695 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.6390413907787932, 'max_depth': 4, 'learning_rate': 0.43803816412712904, 'reg_alpha': 6, 'reg_lambda': 62, 'subsample': 0.9994219720883526}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:55:04,251]\u001b[0m Trial 211 finished with value: -1.1345129682677655 and parameters: {'n_estimators': 58, 'colsample_bytree': 0.6997477686315576, 'max_depth': 4, 'learning_rate': 0.4041161062607009, 'reg_alpha': 2, 'reg_lambda': 64, 'subsample': 0.9829852487341186}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:56:16,300]\u001b[0m Trial 212 finished with value: -1.1325442253721798 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.669874342836965, 'max_depth': 4, 'learning_rate': 0.4578228569713996, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9983254601521879}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:57:28,767]\u001b[0m Trial 213 finished with value: -1.1353279770017166 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.6760899733176969, 'max_depth': 4, 'learning_rate': 0.45056400628413507, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.984392774291531}. Best is trial 202 with value: -1.130699397590487.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:58:39,157]\u001b[0m Trial 214 finished with value: -1.1296800116578858 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.6491344758404808, 'max_depth': 4, 'learning_rate': 0.42244307221085475, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.9984413171357}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 04:59:50,457]\u001b[0m Trial 215 finished with value: -1.1319259787909766 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.6548065939367649, 'max_depth': 4, 'learning_rate': 0.43966684671782363, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9986886503784067}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:00:32,297]\u001b[0m Trial 216 finished with value: -1.1810616499212712 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.6578272500097709, 'max_depth': 2, 'learning_rate': 0.4370114567912136, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.9926435920747282}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:01:46,994]\u001b[0m Trial 217 finished with value: -1.1323542753191291 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.6887677103901061, 'max_depth': 4, 'learning_rate': 0.42029527738351824, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9898663838115861}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:02:59,960]\u001b[0m Trial 218 finished with value: -1.14291481729084 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.6740719085509137, 'max_depth': 4, 'learning_rate': 0.38266110706268364, 'reg_alpha': 6, 'reg_lambda': 61, 'subsample': 0.994716763061011}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:04:22,073]\u001b[0m Trial 219 finished with value: -1.1327485452065142 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.6465488707180104, 'max_depth': 4, 'learning_rate': 0.40521585220240675, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9888217123012588}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:05:37,800]\u001b[0m Trial 220 finished with value: -1.145499205026164 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.6844161455022321, 'max_depth': 4, 'learning_rate': 0.3499650325108601, 'reg_alpha': 7, 'reg_lambda': 58, 'subsample': 0.9979516100009763}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:06:51,587]\u001b[0m Trial 221 finished with value: -1.1331829554898865 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.6439553168150663, 'max_depth': 4, 'learning_rate': 0.4175459619022172, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9792966307069132}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:08:09,592]\u001b[0m Trial 222 finished with value: -1.1311192240605226 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.7045094703486188, 'max_depth': 4, 'learning_rate': 0.41210874821759097, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9995559213042499}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:09:30,746]\u001b[0m Trial 223 finished with value: -1.1335629058539558 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.7203536782071376, 'max_depth': 4, 'learning_rate': 0.40309942600303433, 'reg_alpha': 3, 'reg_lambda': 60, 'subsample': 0.9993706701516879}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:10:45,858]\u001b[0m Trial 224 finished with value: -1.1321899110762847 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.6460415685894612, 'max_depth': 4, 'learning_rate': 0.41588141106897175, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.9843337492337672}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:11:59,662]\u001b[0m Trial 225 finished with value: -1.13611143200059 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.687220915046765, 'max_depth': 4, 'learning_rate': 0.4174172152411678, 'reg_alpha': 4, 'reg_lambda': 59, 'subsample': 0.9849201622125032}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:13:17,114]\u001b[0m Trial 226 finished with value: -1.130959982533451 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.6573961345014895, 'max_depth': 4, 'learning_rate': 0.4192592386156514, 'reg_alpha': 2, 'reg_lambda': 57, 'subsample': 0.9832524419758257}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:14:39,621]\u001b[0m Trial 227 finished with value: -1.136234301524464 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.6966382784839219, 'max_depth': 4, 'learning_rate': 0.4209401991431091, 'reg_alpha': 5, 'reg_lambda': 56, 'subsample': 0.9834566801603333}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:16:01,238]\u001b[0m Trial 228 finished with value: -1.1317427014985224 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7063449803096662, 'max_depth': 4, 'learning_rate': 0.3939042188356822, 'reg_alpha': 2, 'reg_lambda': 57, 'subsample': 0.9608150600444177}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:17:18,767]\u001b[0m Trial 229 finished with value: -1.1446221819636961 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.7068020564952725, 'max_depth': 4, 'learning_rate': 0.38768735485188194, 'reg_alpha': 8, 'reg_lambda': 57, 'subsample': 0.9586974453394629}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:18:39,063]\u001b[0m Trial 230 finished with value: -1.1359077001347249 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7418965269364268, 'max_depth': 4, 'learning_rate': 0.37268383729145105, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.9996738816404264}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:19:54,027]\u001b[0m Trial 231 finished with value: -1.1330110553465833 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.6650497852959063, 'max_depth': 4, 'learning_rate': 0.40469885065339917, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.9739186196331973}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:21:15,969]\u001b[0m Trial 232 finished with value: -1.131008835625919 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.6654632218653912, 'max_depth': 4, 'learning_rate': 0.39048419084852726, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9701995475244676}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:22:41,919]\u001b[0m Trial 233 finished with value: -1.130468279205708 and parameters: {'n_estimators': 69, 'colsample_bytree': 0.7186159878320241, 'max_depth': 4, 'learning_rate': 0.3998844349728361, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9676636101018288}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:24:12,153]\u001b[0m Trial 234 finished with value: -1.1323623651029497 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.7236774100058685, 'max_depth': 4, 'learning_rate': 0.34895664938436355, 'reg_alpha': 2, 'reg_lambda': 56, 'subsample': 0.9577641867313668}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:25:45,488]\u001b[0m Trial 235 finished with value: -1.1358088702212092 and parameters: {'n_estimators': 69, 'colsample_bytree': 0.7132547527173941, 'max_depth': 4, 'learning_rate': 0.34584442598200055, 'reg_alpha': 4, 'reg_lambda': 57, 'subsample': 0.9642971743163271}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:27:09,932]\u001b[0m Trial 236 finished with value: -1.129827880314179 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.6993576676342197, 'max_depth': 4, 'learning_rate': 0.38048496536257653, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9993716847912727}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:28:33,294]\u001b[0m Trial 237 finished with value: -1.1387077777030545 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.7200803592788555, 'max_depth': 4, 'learning_rate': 0.37580684565865113, 'reg_alpha': 6, 'reg_lambda': 59, 'subsample': 0.9591219056478646}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:29:55,879]\u001b[0m Trial 238 finished with value: -1.1348385095006694 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.6899395225111632, 'max_depth': 4, 'learning_rate': 0.33228294057993635, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9735478323035814}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:31:19,857]\u001b[0m Trial 239 finished with value: -1.1371098877107557 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.7029940890985042, 'max_depth': 4, 'learning_rate': 0.3728579508925253, 'reg_alpha': 5, 'reg_lambda': 56, 'subsample': 0.9837981430723199}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:32:40,190]\u001b[0m Trial 240 finished with value: -1.1457189997728916 and parameters: {'n_estimators': 69, 'colsample_bytree': 0.7432629378504593, 'max_depth': 4, 'learning_rate': 0.4009987935767597, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.5978208171757554}. Best is trial 214 with value: -1.1296800116578858.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:34:13,106]\u001b[0m Trial 241 finished with value: -1.1275643812906426 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.674683729573235, 'max_depth': 4, 'learning_rate': 0.3860941057194101, 'reg_alpha': 1, 'reg_lambda': 56, 'subsample': 0.9989938707842768}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:35:39,377]\u001b[0m Trial 242 finished with value: -1.1309843691401826 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.7228881902608405, 'max_depth': 4, 'learning_rate': 0.3546604361028592, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9978665807918355}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:37:05,072]\u001b[0m Trial 243 finished with value: -1.1323281391549074 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.7295490641518737, 'max_depth': 4, 'learning_rate': 0.3646405051412923, 'reg_alpha': 3, 'reg_lambda': 58, 'subsample': 0.9861721920408285}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:38:30,170]\u001b[0m Trial 244 finished with value: -1.1344468836633543 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.7201948863246751, 'max_depth': 4, 'learning_rate': 0.3603673299992518, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.9978307320744626}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:39:56,231]\u001b[0m Trial 245 finished with value: -1.1320386715395956 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.7368726669373087, 'max_depth': 4, 'learning_rate': 0.3876995559843153, 'reg_alpha': 3, 'reg_lambda': 56, 'subsample': 0.9979814899212939}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:41:21,877]\u001b[0m Trial 246 finished with value: -1.1420013133831073 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.7487314907753462, 'max_depth': 4, 'learning_rate': 0.34153395952721316, 'reg_alpha': 7, 'reg_lambda': 56, 'subsample': 0.9970108874717437}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:42:46,990]\u001b[0m Trial 247 finished with value: -1.1341941231679495 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.7336433062149051, 'max_depth': 4, 'learning_rate': 0.31282696452093217, 'reg_alpha': 3, 'reg_lambda': 53, 'subsample': 0.9982310052257203}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:44:16,245]\u001b[0m Trial 248 finished with value: -1.1366219073196722 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.7231973708358517, 'max_depth': 4, 'learning_rate': 0.39106464346964737, 'reg_alpha': 5, 'reg_lambda': 55, 'subsample': 0.9856418638098423}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:45:38,758]\u001b[0m Trial 249 finished with value: -1.1314267811135998 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.7041270323987117, 'max_depth': 4, 'learning_rate': 0.366109523070883, 'reg_alpha': 2, 'reg_lambda': 58, 'subsample': 0.9860242825689987}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:47:01,607]\u001b[0m Trial 250 finished with value: -1.1330488972721855 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.6960648254455044, 'max_depth': 4, 'learning_rate': 0.36591833516936023, 'reg_alpha': 3, 'reg_lambda': 57, 'subsample': 0.9981700119769789}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:48:27,631]\u001b[0m Trial 251 finished with value: -1.1380435749970186 and parameters: {'n_estimators': 69, 'colsample_bytree': 0.7078168720592424, 'max_depth': 4, 'learning_rate': 0.3644854901131107, 'reg_alpha': 6, 'reg_lambda': 53, 'subsample': 0.9990698692297815}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:49:51,592]\u001b[0m Trial 252 finished with value: -1.1318840498766427 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.7302879452279822, 'max_depth': 4, 'learning_rate': 0.3810026596466237, 'reg_alpha': 2, 'reg_lambda': 57, 'subsample': 0.9758804496984397}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:51:18,831]\u001b[0m Trial 253 finished with value: -1.143325403126855 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.7691617731119144, 'max_depth': 4, 'learning_rate': 0.3804541155688876, 'reg_alpha': 9, 'reg_lambda': 56, 'subsample': 0.9819098751392323}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:52:43,559]\u001b[0m Trial 254 finished with value: -1.1342912642328247 and parameters: {'n_estimators': 70, 'colsample_bytree': 0.6804706381220883, 'max_depth': 4, 'learning_rate': 0.3554855394464231, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.9809119559461916}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:54:11,576]\u001b[0m Trial 255 finished with value: -1.1310363606225142 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7298352994716956, 'max_depth': 4, 'learning_rate': 0.3892700936541818, 'reg_alpha': 2, 'reg_lambda': 61, 'subsample': 0.9985135833393292}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:55:08,714]\u001b[0m Trial 256 finished with value: -1.1966318392393949 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.7371112019915816, 'max_depth': 4, 'learning_rate': 0.3908160603083165, 'reg_alpha': 6, 'reg_lambda': 61, 'subsample': 0.23619767779682455}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:56:30,762]\u001b[0m Trial 257 finished with value: -1.1324268570881586 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7287669172341092, 'max_depth': 4, 'learning_rate': 0.4204402337059684, 'reg_alpha': 2, 'reg_lambda': 61, 'subsample': 0.9712927423163757}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:57:55,427]\u001b[0m Trial 258 finished with value: -1.1360360793092967 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.705237602635962, 'max_depth': 4, 'learning_rate': 0.3268289621266263, 'reg_alpha': 4, 'reg_lambda': 55, 'subsample': 0.9990741229954669}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 05:59:19,576]\u001b[0m Trial 259 finished with value: -1.130847525405135 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7505627621567119, 'max_depth': 4, 'learning_rate': 0.3821366578494154, 'reg_alpha': 1, 'reg_lambda': 63, 'subsample': 0.9532482649553542}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:00:43,362]\u001b[0m Trial 260 finished with value: -1.1292462324590782 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7875076099237962, 'max_depth': 4, 'learning_rate': 0.3865104415156265, 'reg_alpha': 1, 'reg_lambda': 63, 'subsample': 0.9995140538964327}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:02:06,733]\u001b[0m Trial 261 finished with value: -1.1282458140438756 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7747086306850732, 'max_depth': 4, 'learning_rate': 0.38504256398373166, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9803156626999046}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:03:30,722]\u001b[0m Trial 262 finished with value: -1.1291314044850664 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7466337638827972, 'max_depth': 4, 'learning_rate': 0.3916460183326384, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.95155688852588}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:04:57,002]\u001b[0m Trial 263 finished with value: -1.1288862521622152 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7846678042513585, 'max_depth': 4, 'learning_rate': 0.3926064429017192, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9457658798276247}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:06:37,944]\u001b[0m Trial 264 finished with value: -1.1300042811462605 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7615111540646653, 'max_depth': 5, 'learning_rate': 0.3800647497363398, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.943708136896529}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:09:19,591]\u001b[0m Trial 265 finished with value: -1.131050317137482 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7814327424054346, 'max_depth': 5, 'learning_rate': 0.3811440371420705, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9462311243936612}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:11:30,332]\u001b[0m Trial 266 finished with value: -1.1411253253740914 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7831118459328219, 'max_depth': 6, 'learning_rate': 0.38948254646346825, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9441367188586313}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:13:16,338]\u001b[0m Trial 267 finished with value: -1.1297276288940137 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7598695315391788, 'max_depth': 5, 'learning_rate': 0.37524918918853706, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9468059122042805}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:14:59,931]\u001b[0m Trial 268 finished with value: -1.1299691601014832 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7899244340065357, 'max_depth': 5, 'learning_rate': 0.37682839935915435, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9511038452411988}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:16:41,086]\u001b[0m Trial 269 finished with value: -1.1306547955072774 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7887150972217617, 'max_depth': 5, 'learning_rate': 0.3802342696526763, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9403726074608177}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:18:23,439]\u001b[0m Trial 270 finished with value: -1.1291849603796613 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7920118359476251, 'max_depth': 5, 'learning_rate': 0.37416407819362657, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.941772899075924}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:20:04,181]\u001b[0m Trial 271 finished with value: -1.129903889517111 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7927180860395752, 'max_depth': 5, 'learning_rate': 0.36313232031186216, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9427059803066395}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:21:48,038]\u001b[0m Trial 272 finished with value: -1.1276897282692215 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8199070757488212, 'max_depth': 5, 'learning_rate': 0.3438079509919994, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9351145532863484}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:23:36,358]\u001b[0m Trial 273 finished with value: -1.1290288782255486 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7973279102495305, 'max_depth': 5, 'learning_rate': 0.33911383906512255, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9401120646709221}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:25:25,422]\u001b[0m Trial 274 finished with value: -1.128625853823771 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8000882181837948, 'max_depth': 5, 'learning_rate': 0.33192538284716566, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9355536835255769}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:27:08,324]\u001b[0m Trial 275 finished with value: -1.1297099940212036 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8012211705919672, 'max_depth': 5, 'learning_rate': 0.33971621689584175, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9357837004600564}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:28:49,772]\u001b[0m Trial 276 finished with value: -1.12995352776247 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8047930858457842, 'max_depth': 5, 'learning_rate': 0.30216027690932323, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9342654724827998}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:30:30,695]\u001b[0m Trial 277 finished with value: -1.132112939268637 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.79842419684391, 'max_depth': 5, 'learning_rate': 0.28163905856573923, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9312086255231644}. Best is trial 241 with value: -1.1275643812906426.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:32:15,490]\u001b[0m Trial 278 finished with value: -1.12729424898248 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.821648033557072, 'max_depth': 5, 'learning_rate': 0.3187670014178026, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9354844390272472}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:34:13,444]\u001b[0m Trial 279 finished with value: -1.1282008993191874 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8228229892264682, 'max_depth': 5, 'learning_rate': 0.31932253882427314, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9363023259951494}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:35:59,312]\u001b[0m Trial 280 finished with value: -1.1281074779154745 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8283246989093033, 'max_depth': 5, 'learning_rate': 0.3031134466088687, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9358155819927549}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:37:44,137]\u001b[0m Trial 281 finished with value: -1.1288595477274317 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.832838193548772, 'max_depth': 5, 'learning_rate': 0.2961477864642699, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9309438710821367}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:39:25,383]\u001b[0m Trial 282 finished with value: -1.1291402750553399 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8291786061871769, 'max_depth': 5, 'learning_rate': 0.3018680421064628, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9028259669801729}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:41:12,644]\u001b[0m Trial 283 finished with value: -1.1292054834758678 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8268963842265189, 'max_depth': 5, 'learning_rate': 0.2997857008797934, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9295669443954847}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:43:00,597]\u001b[0m Trial 284 finished with value: -1.1289273779491054 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8318317115877037, 'max_depth': 5, 'learning_rate': 0.3104844364423925, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.901389279975007}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:44:49,615]\u001b[0m Trial 285 finished with value: -1.1301128713698616 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8150281161855888, 'max_depth': 5, 'learning_rate': 0.3039945200903528, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.8956505710704893}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:46:34,556]\u001b[0m Trial 286 finished with value: -1.1304068915107812 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8324759839363662, 'max_depth': 5, 'learning_rate': 0.30261101599909895, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.8994404748410483}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:48:19,137]\u001b[0m Trial 287 finished with value: -1.1300217744989716 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8190002534532402, 'max_depth': 5, 'learning_rate': 0.29198327123060874, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9038132897334004}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:49:57,971]\u001b[0m Trial 288 finished with value: -1.220091039533465 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8451687221984882, 'max_depth': 5, 'learning_rate': 0.31964431941829197, 'reg_alpha': 43, 'reg_lambda': 64, 'subsample': 0.9198675589260858}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:51:47,913]\u001b[0m Trial 289 finished with value: -1.1317637271666954 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8036063477001749, 'max_depth': 5, 'learning_rate': 0.2694691404604578, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9096300297716329}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:53:42,217]\u001b[0m Trial 290 finished with value: -1.1302654346552714 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8244128149039752, 'max_depth': 5, 'learning_rate': 0.2933831370827855, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9319873015785728}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:55:40,736]\u001b[0m Trial 291 finished with value: -1.1341900841902655 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8077520109043836, 'max_depth': 5, 'learning_rate': 0.2516924689203769, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9089452276319717}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:57:56,659]\u001b[0m Trial 292 finished with value: -1.1279011363113185 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8256490163458642, 'max_depth': 5, 'learning_rate': 0.3221883613327726, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9283411225002917}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 06:59:43,294]\u001b[0m Trial 293 finished with value: -1.127950686455896 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8394733736410863, 'max_depth': 5, 'learning_rate': 0.3309844780440696, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9312098229370546}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:01:41,837]\u001b[0m Trial 294 finished with value: -1.1282052328441572 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8448683662023508, 'max_depth': 5, 'learning_rate': 0.32509976718590233, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.9279675882903194}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:03:34,410]\u001b[0m Trial 295 finished with value: -1.1279947446104377 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8436499766259201, 'max_depth': 5, 'learning_rate': 0.32226915376850357, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.926614536780698}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:05:33,392]\u001b[0m Trial 296 finished with value: -1.1834901780620586 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8383009846747522, 'max_depth': 5, 'learning_rate': 0.32526584713570955, 'reg_alpha': 27, 'reg_lambda': 62, 'subsample': 0.9210924553512355}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:07:38,481]\u001b[0m Trial 297 finished with value: -1.1286479627638872 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8662036285084919, 'max_depth': 5, 'learning_rate': 0.33066570293280917, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.9244594482437254}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:09:38,639]\u001b[0m Trial 298 finished with value: -1.1288886988371611 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8720200146726921, 'max_depth': 5, 'learning_rate': 0.3159137628501656, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.9222794078610758}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:11:27,089]\u001b[0m Trial 299 finished with value: -1.130298031712627 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8689071237959229, 'max_depth': 5, 'learning_rate': 0.33127738921866184, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.9208387308070786}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:13:13,541]\u001b[0m Trial 300 finished with value: -1.1297171576155915 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8520982525906551, 'max_depth': 5, 'learning_rate': 0.31390141238013297, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.9033112397367499}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:15:20,455]\u001b[0m Trial 301 finished with value: -1.1342393204914651 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8489199794764174, 'max_depth': 6, 'learning_rate': 0.3132001715702111, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.8923810421551297}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:17:06,190]\u001b[0m Trial 302 finished with value: -1.1289341115603837 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8587970985616641, 'max_depth': 5, 'learning_rate': 0.3336616019811403, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9110466129024777}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:18:54,697]\u001b[0m Trial 303 finished with value: -1.1390507624255701 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.827708096067458, 'max_depth': 5, 'learning_rate': 0.27315486482617, 'reg_alpha': 4, 'reg_lambda': 68, 'subsample': 0.9250678458795409}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:20:45,387]\u001b[0m Trial 304 finished with value: -1.1290317395701603 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8842888048781834, 'max_depth': 5, 'learning_rate': 0.3370560071802655, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.91120575367697}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:22:34,879]\u001b[0m Trial 305 finished with value: -1.129487628659852 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8651485945648675, 'max_depth': 5, 'learning_rate': 0.3307768303876215, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9062889221479586}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:24:18,589]\u001b[0m Trial 306 finished with value: -1.281731773564473 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8885349067028896, 'max_depth': 5, 'learning_rate': 0.3327064824354516, 'reg_alpha': 70, 'reg_lambda': 69, 'subsample': 0.8818546990221036}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:26:22,988]\u001b[0m Trial 307 finished with value: -1.1382922324836544 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.86597597339461, 'max_depth': 5, 'learning_rate': 0.2849173191220754, 'reg_alpha': 4, 'reg_lambda': 66, 'subsample': 0.9127058788141557}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:28:06,486]\u001b[0m Trial 308 finished with value: -1.1304771019076423 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8761688164495903, 'max_depth': 5, 'learning_rate': 0.31759795758942794, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.8997996310596503}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:29:52,171]\u001b[0m Trial 309 finished with value: -1.1361395249269233 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8272315758987934, 'max_depth': 5, 'learning_rate': 0.2320966847662212, 'reg_alpha': 0, 'reg_lambda': 68, 'subsample': 0.9211198981852472}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:31:39,544]\u001b[0m Trial 310 finished with value: -1.1359846785731358 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.843463737583436, 'max_depth': 5, 'learning_rate': 0.3386143511890195, 'reg_alpha': 4, 'reg_lambda': 66, 'subsample': 0.8899912261031413}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:33:29,786]\u001b[0m Trial 311 finished with value: -1.1305279566250865 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.9039892575666324, 'max_depth': 5, 'learning_rate': 0.3009950183788932, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9126049534241463}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:35:19,947]\u001b[0m Trial 312 finished with value: -1.1361856496103708 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8615319145706032, 'max_depth': 5, 'learning_rate': 0.3246575411157553, 'reg_alpha': 4, 'reg_lambda': 62, 'subsample': 0.9294725872147088}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:37:04,308]\u001b[0m Trial 313 finished with value: -1.130859177260923 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.837007073835987, 'max_depth': 5, 'learning_rate': 0.3389755265283513, 'reg_alpha': 0, 'reg_lambda': 69, 'subsample': 0.9052194702034487}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:39:13,095]\u001b[0m Trial 314 finished with value: -1.137922940620743 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8170893015098925, 'max_depth': 6, 'learning_rate': 0.31160742072541137, 'reg_alpha': 2, 'reg_lambda': 66, 'subsample': 0.8693607776358517}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:40:53,616]\u001b[0m Trial 315 finished with value: -1.1308417840770724 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8564818211823975, 'max_depth': 5, 'learning_rate': 0.28883251639157037, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9283187009710921}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:42:39,125]\u001b[0m Trial 316 finished with value: -1.1349729895962795 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8782178726559771, 'max_depth': 5, 'learning_rate': 0.3446673285063989, 'reg_alpha': 4, 'reg_lambda': 62, 'subsample': 0.9182714069575194}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:44:30,299]\u001b[0m Trial 317 finished with value: -1.1290643041981248 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8366771351163642, 'max_depth': 5, 'learning_rate': 0.32236818122223065, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.8935308004903697}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:47:06,251]\u001b[0m Trial 318 finished with value: -1.1361371617608642 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8330725317393975, 'max_depth': 6, 'learning_rate': 0.2675147395918884, 'reg_alpha': 2, 'reg_lambda': 62, 'subsample': 0.8900430546692485}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:49:41,372]\u001b[0m Trial 319 finished with value: -1.1279905093426326 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8139447336920496, 'max_depth': 5, 'learning_rate': 0.3048819725439586, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.9346168594924479}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:52:18,566]\u001b[0m Trial 320 finished with value: -1.128842277214706 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8178506725641311, 'max_depth': 5, 'learning_rate': 0.2955489493406037, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.929640266261263}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:54:39,581]\u001b[0m Trial 321 finished with value: -1.134858538971009 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.813960240247742, 'max_depth': 5, 'learning_rate': 0.31699740769553647, 'reg_alpha': 4, 'reg_lambda': 61, 'subsample': 0.9348151029019809}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:56:51,249]\u001b[0m Trial 322 finished with value: -1.1338843070850644 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8469229981203814, 'max_depth': 5, 'learning_rate': 0.28757514161167386, 'reg_alpha': 2, 'reg_lambda': 61, 'subsample': 0.9108505244317986}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 07:58:36,631]\u001b[0m Trial 323 finished with value: -1.1323888542842084 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8182937681028372, 'max_depth': 5, 'learning_rate': 0.30872847262390984, 'reg_alpha': 2, 'reg_lambda': 63, 'subsample': 0.933719579642043}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:00:21,471]\u001b[0m Trial 324 finished with value: -1.1289728176075866 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8400033148090169, 'max_depth': 5, 'learning_rate': 0.35003174189639163, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.88941299254714}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:02:08,682]\u001b[0m Trial 325 finished with value: -1.1378910538239007 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8405448729897421, 'max_depth': 5, 'learning_rate': 0.3266330512118492, 'reg_alpha': 5, 'reg_lambda': 61, 'subsample': 0.8587949353818056}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:04:10,311]\u001b[0m Trial 326 finished with value: -1.1306064403369425 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.89195204800521, 'max_depth': 5, 'learning_rate': 0.3510834152530245, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.8808465694854564}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:05:52,158]\u001b[0m Trial 327 finished with value: -1.1361580126547843 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8533872365579994, 'max_depth': 5, 'learning_rate': 0.28029597135133955, 'reg_alpha': 3, 'reg_lambda': 61, 'subsample': 0.8913075933056718}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:07:37,291]\u001b[0m Trial 328 finished with value: -1.1295644697592149 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.824181428397646, 'max_depth': 5, 'learning_rate': 0.33697276392121056, 'reg_alpha': 0, 'reg_lambda': 68, 'subsample': 0.9017795170875624}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:09:24,848]\u001b[0m Trial 329 finished with value: -1.1292836749822188 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.9173236903238566, 'max_depth': 5, 'learning_rate': 0.3009657591067105, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9257369112695268}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:11:29,252]\u001b[0m Trial 330 finished with value: -1.1436827819009139 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8092747819148692, 'max_depth': 6, 'learning_rate': 0.3179438032956363, 'reg_alpha': 6, 'reg_lambda': 62, 'subsample': 0.9160759482274087}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:13:19,674]\u001b[0m Trial 331 finished with value: -1.1338126158901873 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8360188768165447, 'max_depth': 5, 'learning_rate': 0.2595154885892551, 'reg_alpha': 2, 'reg_lambda': 61, 'subsample': 0.8991672679767232}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:15:11,880]\u001b[0m Trial 332 finished with value: -1.1370572668891448 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8773138264358166, 'max_depth': 5, 'learning_rate': 0.3485670016126333, 'reg_alpha': 4, 'reg_lambda': 67, 'subsample': 0.8767956184160425}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:16:55,103]\u001b[0m Trial 333 finished with value: -1.1296834499088488 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7743163818927874, 'max_depth': 5, 'learning_rate': 0.32103964054362755, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.927106769007713}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:18:37,279]\u001b[0m Trial 334 finished with value: -1.1336794234788843 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.854416856671501, 'max_depth': 5, 'learning_rate': 0.2960051730381361, 'reg_alpha': 2, 'reg_lambda': 65, 'subsample': 0.9143456726314652}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:20:17,446]\u001b[0m Trial 335 finished with value: -1.129634303031623 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8059301144944266, 'max_depth': 5, 'learning_rate': 0.34124174603621504, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9394395718767944}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:21:59,441]\u001b[0m Trial 336 finished with value: -1.1339835693258573 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8363909587910006, 'max_depth': 5, 'learning_rate': 0.3102462304577153, 'reg_alpha': 3, 'reg_lambda': 63, 'subsample': 0.8923056801750592}. Best is trial 278 with value: -1.12729424898248.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:23:51,449]\u001b[0m Trial 337 finished with value: -1.1265785730460733 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8234604519735265, 'max_depth': 5, 'learning_rate': 0.3495442574760737, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9138067921086103}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:25:48,629]\u001b[0m Trial 338 finished with value: -1.1321839033559389 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8144593748517301, 'max_depth': 5, 'learning_rate': 0.3471670407763611, 'reg_alpha': 2, 'reg_lambda': 70, 'subsample': 0.9245727998102676}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:27:34,694]\u001b[0m Trial 339 finished with value: -1.1398368271444983 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8617336369306945, 'max_depth': 5, 'learning_rate': 0.32656103095819955, 'reg_alpha': 6, 'reg_lambda': 67, 'subsample': 0.9422528678190802}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:29:20,115]\u001b[0m Trial 340 finished with value: -1.1299054607405408 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8467320550281371, 'max_depth': 5, 'learning_rate': 0.3515801792160777, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.9194990419066807}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:31:26,001]\u001b[0m Trial 341 finished with value: -1.1385901289944087 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.7775460511458991, 'max_depth': 6, 'learning_rate': 0.2805968247367257, 'reg_alpha': 4, 'reg_lambda': 65, 'subsample': 0.9422231556120454}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:33:09,540]\u001b[0m Trial 342 finished with value: -1.1311374711399944 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7988085097372554, 'max_depth': 5, 'learning_rate': 0.3540927928554906, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9140873566414537}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:35:07,974]\u001b[0m Trial 343 finished with value: -1.1310778509800148 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8219750871384779, 'max_depth': 5, 'learning_rate': 0.32861322986918057, 'reg_alpha': 2, 'reg_lambda': 63, 'subsample': 0.9333577379628113}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:36:54,698]\u001b[0m Trial 344 finished with value: -1.1337702702157229 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8840158954124923, 'max_depth': 5, 'learning_rate': 0.2383015347109086, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9479610446496146}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:38:42,026]\u001b[0m Trial 345 finished with value: -1.1358809460310462 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8369458048484986, 'max_depth': 5, 'learning_rate': 0.3346317460996848, 'reg_alpha': 4, 'reg_lambda': 62, 'subsample': 0.8878150341748785}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:40:32,854]\u001b[0m Trial 346 finished with value: -1.13231400063059 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.8556069343100137, 'max_depth': 5, 'learning_rate': 0.3124523926519135, 'reg_alpha': 2, 'reg_lambda': 68, 'subsample': 0.9075578754312651}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:41:43,902]\u001b[0m Trial 347 finished with value: -1.2449214295274014 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8056898235347625, 'max_depth': 5, 'learning_rate': 0.35892903374756857, 'reg_alpha': 54, 'reg_lambda': 65, 'subsample': 0.9303703827102219}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:43:33,891]\u001b[0m Trial 348 finished with value: -1.1309091880327955 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8670317546782207, 'max_depth': 5, 'learning_rate': 0.2871509670986706, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.8718093493055662}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:45:19,327]\u001b[0m Trial 349 finished with value: -1.130132469992264 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8191792452388351, 'max_depth': 5, 'learning_rate': 0.3235151960897475, 'reg_alpha': 2, 'reg_lambda': 64, 'subsample': 0.9434384133452898}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:47:02,493]\u001b[0m Trial 350 finished with value: -1.1389908464053753 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8437428524124877, 'max_depth': 5, 'learning_rate': 0.3411952149482416, 'reg_alpha': 6, 'reg_lambda': 62, 'subsample': 0.9170533075491978}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:48:28,367]\u001b[0m Trial 351 finished with value: -1.2177452909201052 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7970097604061765, 'max_depth': 5, 'learning_rate': 0.3042452044539197, 'reg_alpha': 38, 'reg_lambda': 69, 'subsample': 0.8491607306034613}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:50:37,212]\u001b[0m Trial 352 finished with value: -1.1345410576230477 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.8301034297082057, 'max_depth': 6, 'learning_rate': 0.2711446997518032, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.901421918126808}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:52:24,166]\u001b[0m Trial 353 finished with value: -1.1345214895597449 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8985244241400915, 'max_depth': 5, 'learning_rate': 0.35848107793670825, 'reg_alpha': 4, 'reg_lambda': 60, 'subsample': 0.9502911508756031}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:54:13,352]\u001b[0m Trial 354 finished with value: -1.1308691086707685 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7785086195489962, 'max_depth': 5, 'learning_rate': 0.32782275641296144, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9269811000564169}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:56:01,154]\u001b[0m Trial 355 finished with value: -1.1313819723173906 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8141997288069022, 'max_depth': 5, 'learning_rate': 0.30962160469455935, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9317275857157303}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:57:41,825]\u001b[0m Trial 356 finished with value: -1.1290972699379076 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8714338929212405, 'max_depth': 5, 'learning_rate': 0.3407201276897411, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9120231318313513}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 08:59:23,048]\u001b[0m Trial 357 finished with value: -1.1336481061175168 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8745892245853375, 'max_depth': 5, 'learning_rate': 0.34047322825692383, 'reg_alpha': 2, 'reg_lambda': 66, 'subsample': 0.8876546269805015}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:01:06,957]\u001b[0m Trial 358 finished with value: -1.1375779331913454 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.848105037222923, 'max_depth': 5, 'learning_rate': 0.2949345152642953, 'reg_alpha': 4, 'reg_lambda': 70, 'subsample': 0.9097926582804416}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:02:55,458]\u001b[0m Trial 359 finished with value: -1.129344609741484 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8885229091099603, 'max_depth': 5, 'learning_rate': 0.3159094911959313, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.8989800964733318}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:04:24,410]\u001b[0m Trial 360 finished with value: -1.179602565453072 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8686073212380132, 'max_depth': 5, 'learning_rate': 0.35214405143821453, 'reg_alpha': 8, 'reg_lambda': 64, 'subsample': 0.35592029333372766}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:06:02,994]\u001b[0m Trial 361 finished with value: -1.1716843797522736 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8302304860053401, 'max_depth': 5, 'learning_rate': 0.32648103965040487, 'reg_alpha': 22, 'reg_lambda': 61, 'subsample': 0.9203862972282439}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:07:52,180]\u001b[0m Trial 362 finished with value: -1.1303473018745387 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.9282077630089739, 'max_depth': 5, 'learning_rate': 0.2882053549845258, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.8733962287668351}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:09:59,095]\u001b[0m Trial 363 finished with value: -1.1369635680167898 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8573845933767819, 'max_depth': 6, 'learning_rate': 0.3368860305960276, 'reg_alpha': 2, 'reg_lambda': 65, 'subsample': 0.9093322745342822}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:11:49,210]\u001b[0m Trial 364 finished with value: -1.1400986468900283 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8405907220727877, 'max_depth': 5, 'learning_rate': 0.2640469086208642, 'reg_alpha': 5, 'reg_lambda': 60, 'subsample': 0.9387901382437341}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:13:42,714]\u001b[0m Trial 365 finished with value: -1.1290420924188607 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.820321766556533, 'max_depth': 5, 'learning_rate': 0.3059880975535606, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.889544442214627}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:15:29,024]\u001b[0m Trial 366 finished with value: -1.1349706553233572 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8144877542875624, 'max_depth': 5, 'learning_rate': 0.31414559463531716, 'reg_alpha': 3, 'reg_lambda': 68, 'subsample': 0.8884176686866488}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:17:12,195]\u001b[0m Trial 367 finished with value: -1.1311868667115246 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.7988569664371641, 'max_depth': 5, 'learning_rate': 0.29559989209657117, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.8613759495551215}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:18:49,564]\u001b[0m Trial 368 finished with value: -1.1525007774088474 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.8246484556079757, 'max_depth': 5, 'learning_rate': 0.2752982099472568, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.48587765449158243}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:20:34,852]\u001b[0m Trial 369 finished with value: -1.127935807614164 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8421630827522478, 'max_depth': 5, 'learning_rate': 0.3072535550314758, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9267729792781796}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:22:18,019]\u001b[0m Trial 370 finished with value: -1.1422390847952033 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7929372358855726, 'max_depth': 5, 'learning_rate': 0.29166742628258985, 'reg_alpha': 6, 'reg_lambda': 67, 'subsample': 0.9273449959534874}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:24:14,417]\u001b[0m Trial 371 finished with value: -1.1400402002904446 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8506195675962529, 'max_depth': 5, 'learning_rate': 0.25420092707283487, 'reg_alpha': 4, 'reg_lambda': 65, 'subsample': 0.947466665520204}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:26:05,322]\u001b[0m Trial 372 finished with value: -1.1329170722006978 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8086942072750082, 'max_depth': 5, 'learning_rate': 0.3053314313631258, 'reg_alpha': 2, 'reg_lambda': 69, 'subsample': 0.9253146419078847}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:27:51,781]\u001b[0m Trial 373 finished with value: -1.1287352083320765 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.832626719811783, 'max_depth': 5, 'learning_rate': 0.35910556892757683, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9383810849502668}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:29:34,592]\u001b[0m Trial 374 finished with value: -1.1292666127646116 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8432822187713386, 'max_depth': 5, 'learning_rate': 0.36135852492700854, 'reg_alpha': 0, 'reg_lambda': 67, 'subsample': 0.9504283963945563}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:31:41,935]\u001b[0m Trial 375 finished with value: -1.138015026095061 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8264263509192298, 'max_depth': 6, 'learning_rate': 0.35255233021926535, 'reg_alpha': 2, 'reg_lambda': 71, 'subsample': 0.9357686694768896}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:32:48,126]\u001b[0m Trial 376 finished with value: -1.2991987805139538 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.854316450862436, 'max_depth': 5, 'learning_rate': 0.36347638729110776, 'reg_alpha': 82, 'reg_lambda': 66, 'subsample': 0.9506701301747122}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:34:48,784]\u001b[0m Trial 377 finished with value: -1.3978444585962928 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.9020794717774765, 'max_depth': 5, 'learning_rate': 0.04412127128443416, 'reg_alpha': 4, 'reg_lambda': 64, 'subsample': 0.934308785055098}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:36:32,738]\u001b[0m Trial 378 finished with value: -1.1299075958362983 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.7755317986802666, 'max_depth': 5, 'learning_rate': 0.33173548545757514, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.9191718234927544}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:37:32,101]\u001b[0m Trial 379 finished with value: -1.3354598675072227 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8069571023242053, 'max_depth': 5, 'learning_rate': 0.3440916051828396, 'reg_alpha': 64, 'reg_lambda': 63, 'subsample': 0.5296351851979934}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:39:16,230]\u001b[0m Trial 380 finished with value: -1.1310217083298015 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8364608623470766, 'max_depth': 5, 'learning_rate': 0.3259835976358058, 'reg_alpha': 2, 'reg_lambda': 59, 'subsample': 0.9398948066952397}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:41:03,209]\u001b[0m Trial 381 finished with value: -1.12884072895143 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8646118412525715, 'max_depth': 5, 'learning_rate': 0.3615879830779792, 'reg_alpha': 0, 'reg_lambda': 66, 'subsample': 0.9127330509624774}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:42:45,672]\u001b[0m Trial 382 finished with value: -1.1414671244048786 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.863199723617999, 'max_depth': 5, 'learning_rate': 0.35536344330012726, 'reg_alpha': 7, 'reg_lambda': 68, 'subsample': 0.954643782579026}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:44:38,178]\u001b[0m Trial 383 finished with value: -1.1421614166392569 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8246580417845091, 'max_depth': 5, 'learning_rate': 0.3646999690920762, 'reg_alpha': 4, 'reg_lambda': 1, 'subsample': 0.9214413627832527}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:46:15,060]\u001b[0m Trial 384 finished with value: -1.1406116485864812 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8467612470625598, 'max_depth': 5, 'learning_rate': 0.2786517071900052, 'reg_alpha': 0, 'reg_lambda': 62, 'subsample': 0.6520497554379827}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:47:59,791]\u001b[0m Trial 385 finished with value: -1.1324158144752996 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7938000782991417, 'max_depth': 5, 'learning_rate': 0.31525467083510783, 'reg_alpha': 2, 'reg_lambda': 65, 'subsample': 0.9361325936237802}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:49:58,219]\u001b[0m Trial 386 finished with value: -1.1333944559019715 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8141693023421693, 'max_depth': 6, 'learning_rate': 0.30260614572595723, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.9074349368217013}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:51:06,951]\u001b[0m Trial 387 finished with value: -1.1811251775962175 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.26629768511590474, 'max_depth': 5, 'learning_rate': 0.32810466306066033, 'reg_alpha': 3, 'reg_lambda': 61, 'subsample': 0.9505155990142518}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:52:53,304]\u001b[0m Trial 388 finished with value: -1.1369158242175574 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8656677224546803, 'max_depth': 5, 'learning_rate': 0.3608498478678519, 'reg_alpha': 5, 'reg_lambda': 70, 'subsample': 0.9239725668910679}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:54:43,275]\u001b[0m Trial 389 finished with value: -1.1291737460921594 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8339460888413954, 'max_depth': 5, 'learning_rate': 0.34556291975950687, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.9367223342499222}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:56:28,173]\u001b[0m Trial 390 finished with value: -1.132099565249651 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8506923218953107, 'max_depth': 5, 'learning_rate': 0.28770214071762157, 'reg_alpha': 2, 'reg_lambda': 60, 'subsample': 0.9010268464714142}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:58:07,313]\u001b[0m Trial 391 finished with value: -1.1306884523506384 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.7889949096492379, 'max_depth': 5, 'learning_rate': 0.3125058473448737, 'reg_alpha': 0, 'reg_lambda': 68, 'subsample': 0.9536575083454172}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 09:59:53,943]\u001b[0m Trial 392 finished with value: -1.1313081051024227 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8262619361691573, 'max_depth': 5, 'learning_rate': 0.33582115736686585, 'reg_alpha': 2, 'reg_lambda': 63, 'subsample': 0.9211264225880178}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:01:40,844]\u001b[0m Trial 393 finished with value: -1.1286180956516245 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8023609646667332, 'max_depth': 5, 'learning_rate': 0.3156372553187414, 'reg_alpha': 0, 'reg_lambda': 59, 'subsample': 0.9366998113925469}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:03:41,027]\u001b[0m Trial 394 finished with value: -1.1367366133312165 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8785748989878893, 'max_depth': 5, 'learning_rate': 0.27495414963376263, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.907809113112423}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:05:25,973]\u001b[0m Trial 395 finished with value: -1.1421793895937202 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8101562007988443, 'max_depth': 5, 'learning_rate': 0.2947575114511449, 'reg_alpha': 6, 'reg_lambda': 59, 'subsample': 0.9270401375527135}. Best is trial 337 with value: -1.1265785730460733.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:07:16,791]\u001b[0m Trial 396 finished with value: -1.1260202456839794 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.8434032746845715, 'max_depth': 5, 'learning_rate': 0.31835613938385077, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.955344615253341}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:09:27,566]\u001b[0m Trial 397 finished with value: -1.1294823343608484 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.8591477110520948, 'max_depth': 5, 'learning_rate': 0.3154233538525269, 'reg_alpha': 2, 'reg_lambda': 58, 'subsample': 0.953910518015788}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:11:17,098]\u001b[0m Trial 398 finished with value: -1.1321081829988404 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.769597859712984, 'max_depth': 5, 'learning_rate': 0.253284438537458, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.958184546536791}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:13:39,167]\u001b[0m Trial 399 finished with value: -1.1368127544340791 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8282387939728094, 'max_depth': 6, 'learning_rate': 0.30503821291640776, 'reg_alpha': 3, 'reg_lambda': 61, 'subsample': 0.9392176262682499}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:15:19,739]\u001b[0m Trial 400 finished with value: -1.1298843213612568 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8109843689307469, 'max_depth': 5, 'learning_rate': 0.3187080365731931, 'reg_alpha': 0, 'reg_lambda': 63, 'subsample': 0.953096071133165}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:16:31,525]\u001b[0m Trial 401 finished with value: -1.331085906216428 and parameters: {'n_estimators': 70, 'colsample_bytree': 0.8433808433609821, 'max_depth': 5, 'learning_rate': 0.28249950429369164, 'reg_alpha': 98, 'reg_lambda': 59, 'subsample': 0.9366828501271642}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:18:17,471]\u001b[0m Trial 402 finished with value: -1.1333172452710727 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8671453357646908, 'max_depth': 5, 'learning_rate': 0.29815202452222156, 'reg_alpha': 2, 'reg_lambda': 67, 'subsample': 0.9604525948326337}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:20:04,697]\u001b[0m Trial 403 finished with value: -1.1364438242106298 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8203555070208413, 'max_depth': 5, 'learning_rate': 0.31842614667901264, 'reg_alpha': 5, 'reg_lambda': 61, 'subsample': 0.922632565218571}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:21:59,168]\u001b[0m Trial 404 finished with value: -1.1291521673891574 and parameters: {'n_estimators': 69, 'colsample_bytree': 0.8554505294306504, 'max_depth': 5, 'learning_rate': 0.36454081335671373, 'reg_alpha': 0, 'reg_lambda': 65, 'subsample': 0.936445544907824}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:23:43,102]\u001b[0m Trial 405 finished with value: -1.1464636830015182 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.7840858512773545, 'max_depth': 5, 'learning_rate': 0.3248894397729782, 'reg_alpha': 8, 'reg_lambda': 64, 'subsample': 0.9138550427620246}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:25:22,112]\u001b[0m Trial 406 finished with value: -1.230482924616974 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.837511297482899, 'max_depth': 5, 'learning_rate': 0.26606459267505517, 'reg_alpha': 48, 'reg_lambda': 59, 'subsample': 0.9503198531590931}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:27:09,786]\u001b[0m Trial 407 finished with value: -1.2032466051007276 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8063627892561119, 'max_depth': 6, 'learning_rate': 0.22609995478753986, 'reg_alpha': 32, 'reg_lambda': 62, 'subsample': 0.9257750643912597}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:29:03,413]\u001b[0m Trial 408 finished with value: -1.1319732989800815 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8786388423723703, 'max_depth': 5, 'learning_rate': 0.29648829192312154, 'reg_alpha': 2, 'reg_lambda': 66, 'subsample': 0.9572371956590547}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:30:07,163]\u001b[0m Trial 409 finished with value: -1.322432029331577 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8221668545268266, 'max_depth': 5, 'learning_rate': 0.3368791938729868, 'reg_alpha': 92, 'reg_lambda': 63, 'subsample': 0.9117616969641762}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:31:51,008]\u001b[0m Trial 410 finished with value: -1.1282804230458996 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.84509046246094, 'max_depth': 5, 'learning_rate': 0.31119389785626095, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.9384988850834793}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:33:42,426]\u001b[0m Trial 411 finished with value: -1.1370567110739394 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8364391302648685, 'max_depth': 5, 'learning_rate': 0.2798000255261314, 'reg_alpha': 4, 'reg_lambda': 60, 'subsample': 0.9444146357349859}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:35:29,238]\u001b[0m Trial 412 finished with value: -1.1496211901553393 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7975949731302234, 'max_depth': 5, 'learning_rate': 0.3130863363917201, 'reg_alpha': 11, 'reg_lambda': 59, 'subsample': 0.9591835292822971}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:37:10,104]\u001b[0m Trial 413 finished with value: -1.1284716564478539 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8470051859732091, 'max_depth': 5, 'learning_rate': 0.29627444904727895, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.936817550197128}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:38:51,383]\u001b[0m Trial 414 finished with value: -1.1336256936517255 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8508222896222909, 'max_depth': 5, 'learning_rate': 0.28860242312445905, 'reg_alpha': 2, 'reg_lambda': 61, 'subsample': 0.9374018542264193}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:40:34,989]\u001b[0m Trial 415 finished with value: -1.1286646966265255 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8156050389219868, 'max_depth': 5, 'learning_rate': 0.3672171771511195, 'reg_alpha': 0, 'reg_lambda': 57, 'subsample': 0.9429250755388379}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:42:15,289]\u001b[0m Trial 416 finished with value: -1.1355095822694654 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8098853025585404, 'max_depth': 5, 'learning_rate': 0.3611819784440576, 'reg_alpha': 4, 'reg_lambda': 57, 'subsample': 0.9521318590601042}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:45:37,892]\u001b[0m Trial 417 finished with value: -1.2010619593859446 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.7669858520854961, 'max_depth': 10, 'learning_rate': 0.37251913666459513, 'reg_alpha': 0, 'reg_lambda': 57, 'subsample': 0.9604471838173401}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:47:22,890]\u001b[0m Trial 418 finished with value: -1.132902040647021 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.7924369975062898, 'max_depth': 5, 'learning_rate': 0.2672564115241093, 'reg_alpha': 2, 'reg_lambda': 58, 'subsample': 0.9658049702623166}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:49:27,474]\u001b[0m Trial 419 finished with value: -1.1358860602034513 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8190883347408829, 'max_depth': 6, 'learning_rate': 0.3441298900433085, 'reg_alpha': 0, 'reg_lambda': 54, 'subsample': 0.9399903338062715}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:51:08,644]\u001b[0m Trial 420 finished with value: -1.13865560515022 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8356126681007241, 'max_depth': 5, 'learning_rate': 0.36336591477719193, 'reg_alpha': 6, 'reg_lambda': 59, 'subsample': 0.9397385348918106}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:52:55,312]\u001b[0m Trial 421 finished with value: -1.1308480299784054 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8023260901413222, 'max_depth': 5, 'learning_rate': 0.29736306976931653, 'reg_alpha': 2, 'reg_lambda': 57, 'subsample': 0.9622097906481795}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:54:40,054]\u001b[0m Trial 422 finished with value: -1.1354965035062197 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.7827852997443653, 'max_depth': 5, 'learning_rate': 0.32985573097269916, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.7420514291448913}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:56:32,759]\u001b[0m Trial 423 finished with value: -1.1338899505742581 and parameters: {'n_estimators': 68, 'colsample_bytree': 0.8182775822034152, 'max_depth': 5, 'learning_rate': 0.3469702695658429, 'reg_alpha': 4, 'reg_lambda': 61, 'subsample': 0.9267134400787244}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 10:58:24,489]\u001b[0m Trial 424 finished with value: -1.1293748537045656 and parameters: {'n_estimators': 67, 'colsample_bytree': 0.8413578809502619, 'max_depth': 5, 'learning_rate': 0.3741364241747797, 'reg_alpha': 0, 'reg_lambda': 64, 'subsample': 0.9453805955414266}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:00:11,886]\u001b[0m Trial 425 finished with value: -1.1334490775515516 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.826055428012015, 'max_depth': 5, 'learning_rate': 0.2557418992492252, 'reg_alpha': 2, 'reg_lambda': 55, 'subsample': 0.9287936993182312}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:01:59,271]\u001b[0m Trial 426 finished with value: -1.1336116559610283 and parameters: {'n_estimators': 64, 'colsample_bytree': 0.8501801735073324, 'max_depth': 5, 'learning_rate': 0.3259433519947833, 'reg_alpha': 4, 'reg_lambda': 58, 'subsample': 0.9652754339676369}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:03:46,562]\u001b[0m Trial 427 finished with value: -1.1288762166350654 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8043750138500831, 'max_depth': 5, 'learning_rate': 0.3004818532926843, 'reg_alpha': 0, 'reg_lambda': 52, 'subsample': 0.9431397114032448}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:05:16,174]\u001b[0m Trial 428 finished with value: -1.155976089421635 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8137394742389457, 'max_depth': 5, 'learning_rate': 0.2809632058956871, 'reg_alpha': 2, 'reg_lambda': 51, 'subsample': 0.4348470184548071}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:06:58,843]\u001b[0m Trial 429 finished with value: -1.126971543658559 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8333947312512457, 'max_depth': 5, 'learning_rate': 0.3064601402030124, 'reg_alpha': 0, 'reg_lambda': 55, 'subsample': 0.9300517749440149}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:08:38,433]\u001b[0m Trial 430 finished with value: -1.141890828847342 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.846751685360214, 'max_depth': 5, 'learning_rate': 0.3076037438322864, 'reg_alpha': 7, 'reg_lambda': 56, 'subsample': 0.9227182527797853}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:10:20,857]\u001b[0m Trial 431 finished with value: -1.1351988395989192 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8600473183731091, 'max_depth': 5, 'learning_rate': 0.2747396063573909, 'reg_alpha': 3, 'reg_lambda': 54, 'subsample': 0.9308981803022144}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:12:20,213]\u001b[0m Trial 432 finished with value: -1.13391212755214 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8292663004935238, 'max_depth': 6, 'learning_rate': 0.323579849517842, 'reg_alpha': 0, 'reg_lambda': 57, 'subsample': 0.9127177216969523}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:14:20,700]\u001b[0m Trial 433 finished with value: -1.1300316015279779 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8371406275285982, 'max_depth': 5, 'learning_rate': 0.2947562887932035, 'reg_alpha': 2, 'reg_lambda': 55, 'subsample': 0.964091723345288}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:16:07,872]\u001b[0m Trial 434 finished with value: -1.1367888235577597 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8903672114814722, 'max_depth': 5, 'learning_rate': 0.34791420815752805, 'reg_alpha': 5, 'reg_lambda': 59, 'subsample': 0.9326461412405889}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:17:54,063]\u001b[0m Trial 435 finished with value: -1.1289727625639663 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.862793445524924, 'max_depth': 5, 'learning_rate': 0.30593583080324166, 'reg_alpha': 0, 'reg_lambda': 61, 'subsample': 0.9012898288891218}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:19:40,120]\u001b[0m Trial 436 finished with value: -1.1270069030517853 and parameters: {'n_estimators': 65, 'colsample_bytree': 0.8250465332944735, 'max_depth': 5, 'learning_rate': 0.3285092684245529, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9480845947329514}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:21:23,832]\u001b[0m Trial 437 finished with value: -1.1305996088556145 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8174937169790191, 'max_depth': 5, 'learning_rate': 0.33473990295822487, 'reg_alpha': 2, 'reg_lambda': 58, 'subsample': 0.9561127684789031}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:23:15,128]\u001b[0m Trial 438 finished with value: -1.1335996393767225 and parameters: {'n_estimators': 66, 'colsample_bytree': 0.8431564563316718, 'max_depth': 5, 'learning_rate': 0.3509510377346863, 'reg_alpha': 4, 'reg_lambda': 54, 'subsample': 0.9480656201386193}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:25:09,050]\u001b[0m Trial 439 finished with value: -1.1269658864581293 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8241310370884884, 'max_depth': 5, 'learning_rate': 0.32556073029333255, 'reg_alpha': 0, 'reg_lambda': 60, 'subsample': 0.9616308833246842}. Best is trial 396 with value: -1.1260202456839794.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:26:49,834]\u001b[0m Trial 440 finished with value: -1.1257409430193965 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.852454957946415, 'max_depth': 5, 'learning_rate': 0.3274529226184334, 'reg_alpha': 0, 'reg_lambda': 56, 'subsample': 0.9595235466983985}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:28:23,888]\u001b[0m Trial 441 finished with value: -1.162915125458365 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8037414916595549, 'max_depth': 5, 'learning_rate': 0.31875279431388376, 'reg_alpha': 18, 'reg_lambda': 52, 'subsample': 0.9691229455019429}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:30:04,372]\u001b[0m Trial 442 finished with value: -1.1304971779668427 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.825473982298596, 'max_depth': 5, 'learning_rate': 0.33196742211008107, 'reg_alpha': 2, 'reg_lambda': 56, 'subsample': 0.9684343884154132}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:32:05,479]\u001b[0m Trial 443 finished with value: -1.1381251925408125 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8508993344819147, 'max_depth': 6, 'learning_rate': 0.32330800973691587, 'reg_alpha': 6, 'reg_lambda': 54, 'subsample': 0.970738493696824}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:33:55,899]\u001b[0m Trial 444 finished with value: -1.1276644192143486 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8320682427221243, 'max_depth': 5, 'learning_rate': 0.3422588661006646, 'reg_alpha': 0, 'reg_lambda': 57, 'subsample': 0.9557760162696071}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:36:17,364]\u001b[0m Trial 445 finished with value: -1.1460330345199403 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.7936338626985261, 'max_depth': 7, 'learning_rate': 0.3394319793276573, 'reg_alpha': 3, 'reg_lambda': 56, 'subsample': 0.9630294097534489}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:37:55,588]\u001b[0m Trial 446 finished with value: -1.1268426412106616 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.842683387309706, 'max_depth': 5, 'learning_rate': 0.31325504881003396, 'reg_alpha': 0, 'reg_lambda': 53, 'subsample': 0.9524491775334065}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:39:33,316]\u001b[0m Trial 447 finished with value: -1.144556045035579 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.87385117260412, 'max_depth': 5, 'learning_rate': 0.3120297696888555, 'reg_alpha': 9, 'reg_lambda': 54, 'subsample': 0.9709754114420235}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:41:15,308]\u001b[0m Trial 448 finished with value: -1.130620393419046 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8459226161894342, 'max_depth': 5, 'learning_rate': 0.31228835607470157, 'reg_alpha': 2, 'reg_lambda': 52, 'subsample': 0.9541722301092745}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:42:57,107]\u001b[0m Trial 449 finished with value: -1.1328582631301456 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8506601503489339, 'max_depth': 5, 'learning_rate': 0.326286550185924, 'reg_alpha': 4, 'reg_lambda': 53, 'subsample': 0.9615137895867346}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:44:42,088]\u001b[0m Trial 450 finished with value: -1.128961533724973 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8316883906587522, 'max_depth': 5, 'learning_rate': 0.2841761801055808, 'reg_alpha': 0, 'reg_lambda': 56, 'subsample': 0.954517150194975}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:46:21,873]\u001b[0m Trial 451 finished with value: -1.1321063573911005 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8612349646427288, 'max_depth': 5, 'learning_rate': 0.31235598476140597, 'reg_alpha': 2, 'reg_lambda': 58, 'subsample': 0.9479070662497818}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:48:04,989]\u001b[0m Trial 452 finished with value: -1.1259895894268495 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8416156364500476, 'max_depth': 5, 'learning_rate': 0.3378668289981227, 'reg_alpha': 0, 'reg_lambda': 52, 'subsample': 0.9717606048740247}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:49:45,970]\u001b[0m Trial 453 finished with value: -1.13686651439459 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8310253977816168, 'max_depth': 5, 'learning_rate': 0.300916092990548, 'reg_alpha': 6, 'reg_lambda': 50, 'subsample': 0.9653651068900393}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:51:25,559]\u001b[0m Trial 454 finished with value: -1.1262879518319417 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8067737881277519, 'max_depth': 5, 'learning_rate': 0.3433044430391843, 'reg_alpha': 0, 'reg_lambda': 49, 'subsample': 0.9712671598973452}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:53:06,119]\u001b[0m Trial 455 finished with value: -1.1331340696670984 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8451184866175148, 'max_depth': 5, 'learning_rate': 0.3418887478726788, 'reg_alpha': 4, 'reg_lambda': 52, 'subsample': 0.9734793614233803}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:54:18,402]\u001b[0m Trial 456 finished with value: -1.27311652154517 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8222024261037006, 'max_depth': 5, 'learning_rate': 0.34699423776796634, 'reg_alpha': 69, 'reg_lambda': 51, 'subsample': 0.9766007839510692}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:55:54,035]\u001b[0m Trial 457 finished with value: -1.1489339171680104 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8084109771567185, 'max_depth': 5, 'learning_rate': 0.16823963828916366, 'reg_alpha': 0, 'reg_lambda': 48, 'subsample': 0.9738798326217417}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:57:15,117]\u001b[0m Trial 458 finished with value: -1.2488707504741339 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8389911516863846, 'max_depth': 6, 'learning_rate': 0.3166262357631335, 'reg_alpha': 59, 'reg_lambda': 54, 'subsample': 0.9743505986759657}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 11:58:52,876]\u001b[0m Trial 459 finished with value: -1.139419259707838 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8205071764590485, 'max_depth': 5, 'learning_rate': 0.28471272404758724, 'reg_alpha': 2, 'reg_lambda': 48, 'subsample': 0.6959867877119661}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:00:34,643]\u001b[0m Trial 460 finished with value: -1.1268390014645648 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8498993867625663, 'max_depth': 5, 'learning_rate': 0.33055725476059317, 'reg_alpha': 0, 'reg_lambda': 51, 'subsample': 0.9577610233605719}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:02:19,318]\u001b[0m Trial 461 finished with value: -1.1291359875416307 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8788081141115105, 'max_depth': 5, 'learning_rate': 0.3424680086131824, 'reg_alpha': 2, 'reg_lambda': 49, 'subsample': 0.9590152130542798}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:04:12,004]\u001b[0m Trial 462 finished with value: -1.1324095337638447 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.8522231417633029, 'max_depth': 5, 'learning_rate': 0.32944304682833747, 'reg_alpha': 4, 'reg_lambda': 48, 'subsample': 0.9769796558891616}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:05:54,443]\u001b[0m Trial 463 finished with value: -1.1268110476693713 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8582873474794266, 'max_depth': 5, 'learning_rate': 0.34898812244239513, 'reg_alpha': 0, 'reg_lambda': 53, 'subsample': 0.9542943731406619}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:07:41,365]\u001b[0m Trial 464 finished with value: -1.128977801935911 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8846005812754772, 'max_depth': 5, 'learning_rate': 0.35435836197773846, 'reg_alpha': 2, 'reg_lambda': 50, 'subsample': 0.9775682646938105}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:09:24,274]\u001b[0m Trial 465 finished with value: -1.1268858660442662 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8600131296794801, 'max_depth': 5, 'learning_rate': 0.3485937429984578, 'reg_alpha': 0, 'reg_lambda': 51, 'subsample': 0.9570638966669766}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:11:08,235]\u001b[0m Trial 466 finished with value: -1.1358544933715886 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8678318226584975, 'max_depth': 5, 'learning_rate': 0.3712284939628205, 'reg_alpha': 6, 'reg_lambda': 51, 'subsample': 0.9589043299417449}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:13:26,315]\u001b[0m Trial 467 finished with value: -1.1373079300749795 and parameters: {'n_estimators': 69, 'colsample_bytree': 0.8586578591674957, 'max_depth': 6, 'learning_rate': 0.3536134825247343, 'reg_alpha': 0, 'reg_lambda': 47, 'subsample': 0.9766300534480584}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:15:20,819]\u001b[0m Trial 468 finished with value: -1.131526114637184 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8750906955975255, 'max_depth': 5, 'learning_rate': 0.343938742576288, 'reg_alpha': 4, 'reg_lambda': 51, 'subsample': 0.9589304752404415}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:17:11,633]\u001b[0m Trial 469 finished with value: -1.1311647338537916 and parameters: {'n_estimators': 63, 'colsample_bytree': 0.8926296621845847, 'max_depth': 5, 'learning_rate': 0.36898229482418177, 'reg_alpha': 2, 'reg_lambda': 52, 'subsample': 0.9562124782809617}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:18:52,304]\u001b[0m Trial 470 finished with value: -1.1265278070917408 and parameters: {'n_estimators': 62, 'colsample_bytree': 0.836245741421527, 'max_depth': 5, 'learning_rate': 0.3339872294418327, 'reg_alpha': 0, 'reg_lambda': 53, 'subsample': 0.9796598076328409}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:20:29,068]\u001b[0m Trial 471 finished with value: -1.1259140148389348 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.835765952011959, 'max_depth': 5, 'learning_rate': 0.33327418251494917, 'reg_alpha': 0, 'reg_lambda': 49, 'subsample': 0.9689949124679691}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:22:13,978]\u001b[0m Trial 472 finished with value: -1.129270801316241 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.977965954446717, 'max_depth': 5, 'learning_rate': 0.33700796142013995, 'reg_alpha': 2, 'reg_lambda': 49, 'subsample': 0.9802465226409742}. Best is trial 440 with value: -1.1257409430193965.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:24:00,236]\u001b[0m Trial 473 finished with value: -1.1251907869212847 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8336740307528122, 'max_depth': 5, 'learning_rate': 0.3495235615445477, 'reg_alpha': 0, 'reg_lambda': 45, 'subsample': 0.9693760216801887}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:25:49,906]\u001b[0m Trial 474 finished with value: -1.1319524473567673 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.83567027493789, 'max_depth': 5, 'learning_rate': 0.3520897628565431, 'reg_alpha': 4, 'reg_lambda': 44, 'subsample': 0.9800672033399751}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:27:30,289]\u001b[0m Trial 475 finished with value: -1.1262282989215655 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8596224167599781, 'max_depth': 5, 'learning_rate': 0.35320378143091063, 'reg_alpha': 0, 'reg_lambda': 45, 'subsample': 0.9674403128572783}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:29:10,329]\u001b[0m Trial 476 finished with value: -1.1511920382390395 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8633585724089197, 'max_depth': 5, 'learning_rate': 0.3660554704189216, 'reg_alpha': 15, 'reg_lambda': 46, 'subsample': 0.9686713332669062}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:30:14,309]\u001b[0m Trial 477 finished with value: -1.2863888369743166 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8813920655479404, 'max_depth': 5, 'learning_rate': 0.3512392640303863, 'reg_alpha': 76, 'reg_lambda': 47, 'subsample': 0.983859167463369}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:31:55,590]\u001b[0m Trial 478 finished with value: -1.128801248132842 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8960385604958017, 'max_depth': 5, 'learning_rate': 0.34099065476960355, 'reg_alpha': 2, 'reg_lambda': 46, 'subsample': 0.9697695570937231}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:33:43,517]\u001b[0m Trial 479 finished with value: -1.1255906634070085 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.9150757016767482, 'max_depth': 5, 'learning_rate': 0.37275790933222414, 'reg_alpha': 0, 'reg_lambda': 49, 'subsample': 0.9822303010473773}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:35:26,981]\u001b[0m Trial 480 finished with value: -1.133091539892515 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8604433976788406, 'max_depth': 5, 'learning_rate': 0.37559764288553477, 'reg_alpha': 5, 'reg_lambda': 45, 'subsample': 0.9845690359887244}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:37:07,544]\u001b[0m Trial 481 finished with value: -1.1265137425008276 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.912503607009669, 'max_depth': 5, 'learning_rate': 0.36750855454878484, 'reg_alpha': 0, 'reg_lambda': 50, 'subsample': 0.9700340231059843}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:38:30,438]\u001b[0m Trial 482 finished with value: -1.2151596448691628 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.9381813606226305, 'max_depth': 6, 'learning_rate': 0.3951326038026782, 'reg_alpha': 43, 'reg_lambda': 49, 'subsample': 0.9862708601639606}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:40:12,089]\u001b[0m Trial 483 finished with value: -1.1292871012639227 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.8808118337326717, 'max_depth': 5, 'learning_rate': 0.3623467081321925, 'reg_alpha': 2, 'reg_lambda': 50, 'subsample': 0.9683958258259294}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:41:58,255]\u001b[0m Trial 484 finished with value: -1.138943632338992 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.9493935506627671, 'max_depth': 5, 'learning_rate': 0.37665735232641984, 'reg_alpha': 8, 'reg_lambda': 49, 'subsample': 0.9869439995095863}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:43:47,295]\u001b[0m Trial 485 finished with value: -1.129462730787106 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.915742144797898, 'max_depth': 5, 'learning_rate': 0.4012311226409502, 'reg_alpha': 0, 'reg_lambda': 45, 'subsample': 0.9684472460701695}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:45:33,165]\u001b[0m Trial 486 finished with value: -1.1314208969741344 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.955944296630517, 'max_depth': 5, 'learning_rate': 0.3720159182859993, 'reg_alpha': 3, 'reg_lambda': 52, 'subsample': 0.9892576878218033}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:47:14,875]\u001b[0m Trial 487 finished with value: -1.1274826940585871 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.8923266365556647, 'max_depth': 5, 'learning_rate': 0.3627209873295247, 'reg_alpha': 0, 'reg_lambda': 47, 'subsample': 0.964207148721324}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:48:54,282]\u001b[0m Trial 488 finished with value: -1.1257951052619757 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.8927942051776374, 'max_depth': 5, 'learning_rate': 0.37987214019142557, 'reg_alpha': 0, 'reg_lambda': 43, 'subsample': 0.9772839980183144}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:50:34,701]\u001b[0m Trial 489 finished with value: -1.131418969100004 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.9059524559318008, 'max_depth': 5, 'learning_rate': 0.3800872360753135, 'reg_alpha': 4, 'reg_lambda': 44, 'subsample': 0.9887997616433437}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:52:14,877]\u001b[0m Trial 490 finished with value: -1.129895329229052 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.922655580924729, 'max_depth': 5, 'learning_rate': 0.3958568890281929, 'reg_alpha': 2, 'reg_lambda': 41, 'subsample': 0.9762355381053488}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:54:04,206]\u001b[0m Trial 491 finished with value: -1.1356175406497389 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.9036172343824732, 'max_depth': 5, 'learning_rate': 0.4051024804596873, 'reg_alpha': 6, 'reg_lambda': 47, 'subsample': 0.999498930665051}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:55:46,198]\u001b[0m Trial 492 finished with value: -1.1253853055127263 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.9104726927426969, 'max_depth': 5, 'learning_rate': 0.3721898841902834, 'reg_alpha': 0, 'reg_lambda': 47, 'subsample': 0.9745939084275749}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:57:28,330]\u001b[0m Trial 493 finished with value: -1.1252518924178445 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.9345293359224789, 'max_depth': 5, 'learning_rate': 0.3865065772906857, 'reg_alpha': 0, 'reg_lambda': 46, 'subsample': 0.9816828907488533}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 12:59:30,711]\u001b[0m Trial 494 finished with value: -1.139350177076143 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.9037933350582575, 'max_depth': 6, 'learning_rate': 0.40659604354567835, 'reg_alpha': 2, 'reg_lambda': 43, 'subsample': 0.9998286899605436}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 13:01:17,247]\u001b[0m Trial 495 finished with value: -1.1291204382116842 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.9397663311470101, 'max_depth': 5, 'learning_rate': 0.3896735177590513, 'reg_alpha': 0, 'reg_lambda': 47, 'subsample': 0.9787098184267723}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 13:03:02,133]\u001b[0m Trial 496 finished with value: -1.131888876528629 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.9155179295785864, 'max_depth': 5, 'learning_rate': 0.37724175671395993, 'reg_alpha': 4, 'reg_lambda': 45, 'subsample': 0.9845842059764759}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 13:04:54,817]\u001b[0m Trial 497 finished with value: -1.1310606627031445 and parameters: {'n_estimators': 60, 'colsample_bytree': 0.9298408582498872, 'max_depth': 5, 'learning_rate': 0.37631297707770933, 'reg_alpha': 2, 'reg_lambda': 49, 'subsample': 0.9706856859109086}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 13:06:40,927]\u001b[0m Trial 498 finished with value: -1.1266733724677902 and parameters: {'n_estimators': 61, 'colsample_bytree': 0.9667553169643113, 'max_depth': 5, 'learning_rate': 0.393761232507768, 'reg_alpha': 0, 'reg_lambda': 46, 'subsample': 0.9860563647882298}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n",
      "\u001b[32m[I 2021-08-11 13:08:23,967]\u001b[0m Trial 499 finished with value: -1.1293982948779708 and parameters: {'n_estimators': 59, 'colsample_bytree': 0.9904120792491262, 'max_depth': 5, 'learning_rate': 0.4103381353176318, 'reg_alpha': 0, 'reg_lambda': 43, 'subsample': 0.989472344138437}. Best is trial 473 with value: -1.1251907869212847.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12h 18min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "study.optimize( Oprimizer, n_trials=500, timeout=time_it() )    ### BEAT: -1.124290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "496229d3-ec8f-4c9d-b32d-fbbb05dab06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 61,\n",
       " 'colsample_bytree': 0.8336740307528122,\n",
       " 'max_depth': 5,\n",
       " 'learning_rate': 0.3495235615445477,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 45,\n",
       " 'subsample': 0.9693760216801887}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d70d73ff-7922-4725-b79e-7d51930faa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1251907869212847"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e548cd-cbe3-4d8a-981f-8bfdcd7e0859",
   "metadata": {},
   "source": [
    "## Two Model Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "79dacc30-b29d-4f82-82ae-40335e871f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_1 = {\n",
    "    \n",
    "    \"n_estimators\"     : 53,\n",
    "    \"colsample_bytree\" : 0.8,\n",
    "    \"max_depth\"        : 4,\n",
    "    \"n_jobs\"           : 4,\n",
    "    \"random_state\"     : 0,\n",
    "    \"verbosity\"        : 1,\n",
    "    \n",
    "}\n",
    "\n",
    "model_dict_2 = {\n",
    "    \n",
    "    'n_estimators'     : 61,\n",
    "    'colsample_bytree' : 0.8336740307528122,\n",
    "    'max_depth'        : 5,\n",
    "    'learning_rate'    : 0.3495235615445477,\n",
    "    'reg_alpha'        : 0,\n",
    "    'reg_lambda'       : 45,\n",
    "    'subsample'        : 0.9693760216801887,\n",
    "    \"n_jobs\"           : 4,\n",
    "    \"random_state\"     : 0,\n",
    "    \"verbosity\"        : 0,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bc17f585-2449-4a8b-b148-4ae5f1189e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_1 = XGBClassifier(**model_dict_1)\n",
    "Model_2 = XGBClassifier(**model_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4f1ab1e4-4621-41fe-92fa-667664d3cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:   56.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "sKF =  StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "\n",
    "y_peed_prob_1 = cross_val_predict(Model_1, X_train, y_train, cv=sKF, n_jobs=-2, verbose=1, method='predict_proba')\n",
    "y_peed_prob_2 = cross_val_predict(Model_2, X_train, y_train, cv=sKF, n_jobs=-2, verbose=1, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9f853b64-d249-4007-b313-6743e0800e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_bled(zeta):\n",
    "    y_pred_new = (zeta * y_peed_prob_1) + ((1-zeta) * y_peed_prob_2)\n",
    "    LogLoss = log_loss(y_train, y_pred_new)\n",
    "    return LogLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "afaa4078-2611-4530-ae40-b18f94e6fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LoLo = [best_bled(_) for _ in np.linspace(0, 1, 101)]\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 101), LoLo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b7cfd925-c3f1-4b2f-8a52-69373cfbe4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " opt_zeta_val = 0.44\n"
     ]
    }
   ],
   "source": [
    "opt_zeta_val = np.linspace(0, 1, 101)[np.argmin(LoLo)]\n",
    "print(f\"{ opt_zeta_val = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "43254180-9ae3-4995-8370-36326ed2331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:26:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 27.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8336740307528122, gamma=0,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.3495235615445477, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=61, n_jobs=4, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=45, scale_pos_weight=None,\n",
       "              subsample=0.9693760216801887, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=0)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Model_1.fit(X_train, y_train)\n",
    "Model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a68d39a-85fe-4559-80e9-441ca62f6b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred_1 = Model_1.predict_proba(X_test)\n",
    "y_pred_2 = Model_2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "55f23e7a-db2f-4bc9-927e-f76bdb2c2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (opt_zeta_val * y_pred_1) + ((1 - opt_zeta_val) * y_pred_2)\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred, columns=sub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f4aae681-58e9-4c62-8ad7-87c285e2f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acoustic/Folk_0</th>\n",
       "      <th>Alt_Music_1</th>\n",
       "      <th>Blues_2</th>\n",
       "      <th>Bollywood_3</th>\n",
       "      <th>Country_4</th>\n",
       "      <th>HipHop_5</th>\n",
       "      <th>Indie Alt_6</th>\n",
       "      <th>Instrumental_7</th>\n",
       "      <th>Metal_8</th>\n",
       "      <th>Pop_9</th>\n",
       "      <th>Rock_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.069420</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.031276</td>\n",
       "      <td>0.010281</td>\n",
       "      <td>0.850683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.702294</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>0.229862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.045870</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.027530</td>\n",
       "      <td>0.029729</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.708838</td>\n",
       "      <td>0.182573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.059420</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.050046</td>\n",
       "      <td>0.054364</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.028350</td>\n",
       "      <td>0.640215</td>\n",
       "      <td>0.164033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.987743</td>\n",
       "      <td>0.004367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acoustic/Folk_0  Alt_Music_1   Blues_2  Bollywood_3  Country_4  HipHop_5  \\\n",
       "0         0.000130     0.030061  0.004907     0.000136   0.000129  0.002860   \n",
       "1         0.000138     0.009214  0.005542     0.000084   0.000145  0.002476   \n",
       "2         0.000080     0.045870  0.001438     0.000099   0.000222  0.027530   \n",
       "3         0.000192     0.059420  0.002789     0.000164   0.000269  0.050046   \n",
       "4         0.001483     0.000407  0.000215     0.001043   0.003143  0.000174   \n",
       "\n",
       "   Indie Alt_6  Instrumental_7   Metal_8     Pop_9   Rock_10  \n",
       "0     0.069420        0.000114  0.031276  0.010281  0.850683  \n",
       "1     0.041168        0.000091  0.702294  0.008985  0.229862  \n",
       "2     0.029729        0.000082  0.003537  0.708838  0.182573  \n",
       "3     0.054364        0.000157  0.028350  0.640215  0.164033  \n",
       "4     0.000197        0.000116  0.001112  0.987743  0.004367  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689a4d0-25b9-4c2b-85d9-1f66ef42958a",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2b00d359-a4be-49a2-a45d-f6d4588c40d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:58:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Enter Your Param Dict\n",
    "your_param_dict = {\n",
    "    \n",
    "    \"n_estimators\"     : 53,\n",
    "    \"colsample_bytree\" : 0.8,\n",
    "    \"max_depth\"        : 4,\n",
    "    \"n_jobs\"           : 4,\n",
    "    \"random_state\"     : 0,\n",
    "    \"verbosity\"        : 1,\n",
    "    \n",
    "}\n",
    "# From XBG Boost Clf\n",
    "xgb = XGBClassifier(use_label_encoder=False, **your_param_dict)\n",
    "# Train Clf\n",
    "xgb.fit(X_train, y_train)\n",
    "# Predict Results\n",
    "y_pred = pd.DataFrame(xgb.predict_proba(X_test), columns=sub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c594b658-4e52-408c-a437-6e081cbe90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CSV\n",
    "y_pred.to_csv(\"../SUBS/BlendXGB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0c9552a9-34e8-4a4d-904d-805a735b7a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acoustic/Folk_0</th>\n",
       "      <th>Alt_Music_1</th>\n",
       "      <th>Blues_2</th>\n",
       "      <th>Bollywood_3</th>\n",
       "      <th>Country_4</th>\n",
       "      <th>HipHop_5</th>\n",
       "      <th>Indie Alt_6</th>\n",
       "      <th>Instrumental_7</th>\n",
       "      <th>Metal_8</th>\n",
       "      <th>Pop_9</th>\n",
       "      <th>Rock_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.033533</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.071154</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.027218</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.854455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.040746</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.725983</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.206102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.055252</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.034282</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>0.604767</td>\n",
       "      <td>0.266727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.054085</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.025533</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>0.675102</td>\n",
       "      <td>0.185639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.989991</td>\n",
       "      <td>0.005738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acoustic/Folk_0  Alt_Music_1   Blues_2  Bollywood_3  Country_4  HipHop_5  \\\n",
       "0         0.000067     0.033533  0.005682     0.000035   0.000042  0.001737   \n",
       "1         0.000048     0.008289  0.005642     0.000024   0.000053  0.002698   \n",
       "2         0.000032     0.055252  0.001525     0.000058   0.000268  0.034282   \n",
       "3         0.000045     0.054085  0.001554     0.000052   0.000085  0.025533   \n",
       "4         0.000880     0.000060  0.000129     0.000545   0.002008  0.000113   \n",
       "\n",
       "   Indie Alt_6  Instrumental_7   Metal_8     Pop_9   Rock_10  \n",
       "0     0.071154        0.000027  0.027218  0.006050  0.854455  \n",
       "1     0.040746        0.000026  0.725983  0.010389  0.206102  \n",
       "2     0.033879        0.000046  0.003165  0.604767  0.266727  \n",
       "3     0.043895        0.000028  0.013982  0.675102  0.185639  \n",
       "4     0.000069        0.000023  0.000444  0.989991  0.005738  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2838417-0e55-46e0-8584-74cdcbb8cab0",
   "metadata": {},
   "source": [
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
    "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
    "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdee2d8-446f-456e-ab6a-6b2e8536576b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ac5b8-fc5a-42fa-8368-e10384e78271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
